{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46e337f",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Training models to generate midi sequences of piano music\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import glob\n",
    "import random\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from symusic import Score\n",
    "from miditok import REMI, TokenizerConfig, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "\n",
    "from midi2audio import FluidSynth # Import library\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from pretty_midi import PrettyMIDI\n",
    "\n",
    "import random\n",
    "from mido import Message, MidiFile, MidiTrack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bf256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses 'cuda' if a gpu is detected. Otherwise uses cpu\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Can also set manually\n",
    "#DEVICE = 'cpu'\n",
    "#DEVICE = 'cuda'\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata and MIDI file paths from MAESTRO dataset\n",
    "\n",
    "ROOT = \"maestro-v3.0.0\"            # change if you unpacked elsewhere\n",
    "meta = pd.read_csv(os.path.join(ROOT, \"maestro-v3.0.0.csv\"))\n",
    "\n",
    "def list_midi_files(split):\n",
    "    paths = meta.loc[meta[\"split\"] == split, \"midi_filename\"]\n",
    "    return [os.path.join(ROOT, p) for p in paths]\n",
    "\n",
    "train_files = list_midi_files(\"train\")        # 962 MIDI files\n",
    "val_files   = list_midi_files(\"validation\")   # 137\n",
    "test_files  = list_midi_files(\"test\")         # 177\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type validity\n",
    "type(train_files[0])\n",
    "train_files[0].encode('utf-8').decode('utf-8')\n",
    "print(train_files[0].encode('utf-8'))\n",
    "str.encode(train_files[0], 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1fe386",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer parameters\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"use_chords\": True,\n",
    "    \"use_tempos\": True,\n",
    "    \"use_time_signatures\": True,\n",
    "    \"use_key_signatures\": True,\n",
    "}\n",
    "\n",
    "# Create the tokenizer configuration\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "\n",
    "# Initialize the REMI tokenizer with the configuration\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = DatasetMIDI(\n",
    "    files_paths=train_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "test_dataset = DatasetMIDI(\n",
    "    files_paths=test_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35331c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794e05e",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our basic Transformer model for music generation\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, num_heads=8, num_layers=6, dropout=0.1, max_seq_len=1024):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = nn.Parameter(self._generate_positional_encoding(max_seq_len, embedding_dim), requires_grad=False)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embedding_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        x = self.embedding(x) + self.pos_encoder[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_len, d_model):\n",
    "        \"\"\"Creates sinusoidal positional encoding matrix\"\"\"\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097e0ab",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831bd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the dataset\n",
    "def train(model, train_loader, val_loader, vocab_size, num_epochs=20, lr=0.001, device=DEVICE):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --------- Training ---------\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch['input_ids'].to(device)  # (batch_size, seq_length)\n",
    "\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # --------- Validation ---------\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch['input_ids'].to(device)\n",
    "\n",
    "                inputs = batch[:, :-1]\n",
    "                targets = batch[:, 1:]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                targets = targets.reshape(-1)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "\n",
    "model = MusicTransformer(vocab_size, embedding_dim=256, num_heads=8, num_layers=6)\n",
    "train(model, train_loader, test_loader, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb8e86",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f48fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling: Generate new music from the trained model\n",
    "def sample(model, start_token, tokenizer, max_length=512, temperature=1.0, device=DEVICE):\n",
    "    model.eval()\n",
    "\n",
    "    # Build ID ‚Üí string mapping\n",
    "    if hasattr(tokenizer, 'vocab') and isinstance(tokenizer.vocab, dict):\n",
    "        id_to_token = {v: k for k, v in tokenizer.vocab.items()}\n",
    "    elif hasattr(tokenizer, '_vocab'):\n",
    "        id_to_token = {i: tok for i, tok in enumerate(tokenizer._vocab)}\n",
    "    else:\n",
    "        raise RuntimeError(\"Tokenizer vocab not found\")\n",
    "\n",
    "    generated = [start_token]\n",
    "    input_seq = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    while len(generated) < max_length:\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_seq)\n",
    "            next_logits = logits[0, -1] / temperature\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        token_str = id_to_token.get(next_token, \"\")\n",
    "\n",
    "        # Always add bar/position/timeshift\n",
    "        if token_str.startswith((\"Bar\", \"TimeShift\", \"Position\")):\n",
    "            generated.append(next_token)\n",
    "\n",
    "        # If it's a pitch, follow with velocity and duration\n",
    "        elif token_str.startswith(\"Pitch_\"):\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # Sample a Velocity token\n",
    "            velocity_ids = [i for i, tok in id_to_token.items() if tok.startswith(\"Velocity_\")]\n",
    "            generated.append(random.choice(velocity_ids))\n",
    "\n",
    "            # Sample a Duration token\n",
    "            duration_ids = [i for i, tok in id_to_token.items() if tok.startswith(\"Duration_\")]\n",
    "            generated.append(random.choice(duration_ids))\n",
    "\n",
    "        # Stop on EOS or PAD\n",
    "        if token_str in (\"EOS_None\"):\n",
    "            break\n",
    "\n",
    "        input_seq = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save a sample MIDI file\n",
    "start_token = tokenizer.special_tokens_ids[1]\n",
    "generated_sequence = sample(model, start_token, tokenizer, max_length=1024)\n",
    "\n",
    "print(\"Generated token sequence:\")\n",
    "print(generated_sequence)\n",
    "print(\"num tokens:\", len(generated_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cfb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_score = tokenizer.decode([generated_sequence])\n",
    "output_score.dump_midi(f\"transformer.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect generated MIDI content\n",
    "pretty_midi = PrettyMIDI(\"transformer.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d2b72e",
   "metadata": {},
   "source": [
    "Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502cb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly generate a midifile as a baseline\n",
    "def generate_random_midi(filename=\"random.mid\", \n",
    "                         num_notes=50, \n",
    "                         pitch_range=(21, 108), \n",
    "                         velocity_range=(30, 100), \n",
    "                         duration_range=(120, 480)):\n",
    "    mid = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "\n",
    "    time = 0\n",
    "    for _ in range(num_notes):\n",
    "        pitch = random.randint(*pitch_range)\n",
    "        velocity = random.randint(*velocity_range)\n",
    "        duration = random.randint(*duration_range)\n",
    "\n",
    "        # Note on\n",
    "        track.append(Message('note_on', note=pitch, velocity=velocity, time=time))\n",
    "        # Note off\n",
    "        track.append(Message('note_off', note=pitch, velocity=0, time=duration))\n",
    "\n",
    "        time = 0  # subsequent notes start right after the previous ends\n",
    "\n",
    "    mid.save(filename)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "# Generate an example\n",
    "generate_random_midi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb87251",
   "metadata": {},
   "source": [
    "Evaluate the generated midi file against the randomly generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot notes by beat to see if model learned timings\n",
    "def plot_notes_by_beat(beat_note_counts):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Organize data\n",
    "    beats = sorted(beat_note_counts.keys())\n",
    "    all_pitches = sorted({p for counter in beat_note_counts.values() for p in counter})\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot each pitch as scatter points\n",
    "    for pitch in all_pitches:\n",
    "        x = []\n",
    "        y = []\n",
    "        sizes = []\n",
    "        for beat in beats:\n",
    "            count = beat_note_counts[beat].get(pitch, 0)\n",
    "            if count > 0:\n",
    "                x.append(beat)\n",
    "                y.append(pitch)\n",
    "                sizes.append(count * 20)  # Adjust marker size scaling as needed\n",
    "\n",
    "        plt.scatter(x, y, s=sizes, alpha=0.6, label=pitch)\n",
    "\n",
    "    plt.title(\"Note Occurrences by Beat Position\")\n",
    "    plt.xlabel(\"Beat Number in Measure\")\n",
    "    plt.ylabel(\"Pitch\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=\"small\", ncol=2)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "# Pliot note frequencies\n",
    "def plot_note_frequencies(note_list, detected_key):\n",
    "    note_counts = Counter(note_list)\n",
    "    labels, counts = zip(*sorted(note_counts.items(), key=lambda x: x[0]))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, counts, color='skyblue', edgecolor='black')\n",
    "\n",
    "    # Highlight notes in the key\n",
    "    in_key = set(p.name for p in detected_key.pitches)\n",
    "    for bar, label in zip(bars, labels):\n",
    "        if label not in in_key:\n",
    "            bar.set_color('salmon')\n",
    "\n",
    "    plt.title(f\"Note Occurrences in MIDI (Key: {detected_key})\")\n",
    "    plt.xlabel(\"Note Name (no octave)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze midi files against each other\n",
    "def analyze_midi_key_compliance(file_path):\n",
    "    # Parse the MIDI file\n",
    "    try:\n",
    "        midi_stream = converter.parse(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to parse MIDI: {e}\")\n",
    "        return\n",
    "\n",
    "    # Detect key\n",
    "    try:\n",
    "        detected_key = midi_stream.analyze('key')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to detect key: {e}\")\n",
    "        return\n",
    "\n",
    "    allowed_notes = set(p.name for p in detected_key.pitches)\n",
    "\n",
    "    print(f\"\\nüéº Detected Key: {detected_key}\")\n",
    "    print(f\"üéµ Allowed Notes: {sorted(allowed_notes)}\\n\")\n",
    "\n",
    "    note_names = []\n",
    "    non_conforming = []\n",
    "\n",
    "    # Collect note data\n",
    "    for element in midi_stream.recurse():\n",
    "        if isinstance(element, note.Note):\n",
    "            name = element.pitch.name  # e.g. \"C#\", \"A\"\n",
    "            note_names.append(name)\n",
    "            if name not in allowed_notes:\n",
    "                non_conforming.append((name, element.offset))\n",
    "\n",
    "    total_notes = len(note_names)\n",
    "\n",
    "    # Display compliance results\n",
    "    if non_conforming:\n",
    "        print(f\"‚ùå Found {len(non_conforming)} non-conforming notes (out of {total_notes}):\")\n",
    "        for pitch, offset in non_conforming:\n",
    "            print(f\" - {pitch} at offset {offset}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All notes conform to the detected key.\")\n",
    "\n",
    "    # Plot note frequency\n",
    "    plot_note_frequencies(note_names, detected_key)\n",
    "\n",
    "    note_names = []\n",
    "    non_conforming = []\n",
    "    beat_note_counts = defaultdict(Counter)\n",
    "    time_signatures = []\n",
    "    velocity_per_beat = defaultdict(list)\n",
    "    velocity_counts_per_beat = defaultdict(Counter)\n",
    "\n",
    "\n",
    "\n",
    "    for element in midi_stream.recurse():\n",
    "        if isinstance(element, meter.TimeSignature):\n",
    "            time_signatures.append((element.ratioString, element.offset))\n",
    "\n",
    "        if isinstance(element, note.Note):\n",
    "            name = element.pitch.name\n",
    "            beat = round(element.beat, 2)  # round for grouping\n",
    "            note_names.append(name)\n",
    "\n",
    "            if name not in allowed_notes:\n",
    "                non_conforming.append((name, element.offset))\n",
    "\n",
    "            beat_note_counts[beat][name] += 1\n",
    "            velocity_per_beat[beat].append(element.volume.velocity or 64)  # MIDI default = 64\n",
    "            velocity_counts_per_beat[beat][element.volume.velocity] += 1\n",
    "\n",
    "    # Print time signature info\n",
    "    print(\"\\nüïê Time Signature(s) detected:\")\n",
    "    for ts, offset in time_signatures:\n",
    "        print(f\" - {ts} at offset {offset}\")\n",
    "\n",
    "    # ... [existing compliance and plotting code]\n",
    "\n",
    "    print(\"\\nüìä Note occurrences per beat position (aggregated):\")\n",
    "    for beat in sorted(beat_note_counts):\n",
    "        print(f\" Beat {beat}:\")\n",
    "        for pitch, count in beat_note_counts[beat].items():\n",
    "            print(f\"   - {pitch}: {count}\")\n",
    "\n",
    "    plot_notes_by_beat(beat_note_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4be629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a random sample vs a sample from the LSTM\n",
    "\n",
    "#rand_file_path = \"random/filepath.mid\"\n",
    "#analyze_midi_key_compliance(rand_file_path)\n",
    "#gen_file_path = \"generated/filepath.mid\"\n",
    "#analyze_midi_key_compliance(gen_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21305ff",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Training models to generate\n",
    "midi sequences of music\n",
    "conditioned on instruments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e098c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "\n",
    "from collections import defaultdict\n",
    "from symusic import Score\n",
    "\n",
    "from pretty_midi import PrettyMIDI, program_to_instrument_name\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "from music21 import converter, note, key, stream, meter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from music21 import converter, note, instrument\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gpu if found, otherwise use cpu\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "# ------------ Dataset -------------\n",
    "# Download from: https://www.kaggle.com/datasets/imsparsh/lakh-midi-clean?resource=download\n",
    "# Unzip and name dataset folder root \"lakh-midi\"\n",
    "DATA_ROOT = \"lakh-midi\"\n",
    "\n",
    "# If you have a CSV with train/val/test splits, load it here; otherwise we create one.\n",
    "SPLIT_CSV = None  # path to optional CSV with columns [filepath, split]\n",
    "\n",
    "MAX_SEQ_LEN = 1024           # split long pieces into chunks of this many tokens\n",
    "BATCH_SIZE  = 4\n",
    "NUM_EPOCHS  = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "EMBED_DIM  = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan dataset to find top‚Äë20 instruments\n",
    "def collect_program_counts(root: str):\n",
    "    counts = defaultdict(int)\n",
    "    midi_files = [os.path.join(dp, f)\n",
    "                  for dp, _, files in os.walk(root)\n",
    "                  for f in files if f.lower().endswith((\".mid\", \".midi\"))]\n",
    "    for fp in midi_files:\n",
    "        try:\n",
    "            score = Score(fp)\n",
    "            for track in score.tracks:\n",
    "                if track.is_drum:                 # skip channel-10 drums\n",
    "                    continue\n",
    "                counts[track.program] += 1\n",
    "        except Exception:\n",
    "            continue                              # skip unreadable files\n",
    "    return counts, midi_files\n",
    "\n",
    "program_counts, all_midi_files = collect_program_counts(DATA_ROOT)\n",
    "TOP20 = sorted(program_counts.items(), key=lambda kv: kv[1], reverse=True)[:20]\n",
    "allowed_programs = [p for p, _ in TOP20]\n",
    "\n",
    "print(\"Top-20 instruments:\")\n",
    "print(f\"{'GM#':>4}  {'Name':30}  Tracks\")\n",
    "print(\"-\" * 46)\n",
    "for prog, cnt in TOP20:\n",
    "    name = program_to_instrument_name(prog)   # e.g. 0 ‚Üí Acoustic Grand Piano\n",
    "    print(f\"{prog:>4}  {name:30}  {cnt:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMI with Program‚ÄëChange events ‚Üí single flattened event stream\n",
    "config = TokenizerConfig(\n",
    "    use_programs=True,      # ‚Üê this is the key flag\n",
    "    one_token_stream=True,  # keeps your single‚Äëstream RNN forward\n",
    ")\n",
    "\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "# Helper set for fast lookup during constrained sampling\n",
    "PROGRAM_TOKEN_IDS = {\n",
    "    p: tokenizer[f\"Program_{p}\"]\n",
    "    for p in range(128)\n",
    "    if f\"Program_{p}\" in tokenizer  # safety for unseen programs\n",
    "}\n",
    "\n",
    "ALL_PROGRAM_TOKEN_IDS = set(PROGRAM_TOKEN_IDS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba46c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and testsets for training and evaluation\n",
    "def build_split_lists(root: str, csv: Optional[str] = None):\n",
    "    \"\"\"Return lists of MIDI paths grouped by split.\"\"\"\n",
    "    if csv:\n",
    "        meta = pd.read_csv(csv)\n",
    "        train = meta.loc[meta[\"split\"] == \"train\", \"filepath\"].tolist()\n",
    "        val   = meta.loc[meta[\"split\"] == \"validation\", \"filepath\"].tolist()\n",
    "        test  = meta.loc[meta[\"split\"] == \"test\", \"filepath\"].tolist()\n",
    "    else:\n",
    "        # Simple 80‚Äë10‚Äë10 random split over *.mid files in the root\n",
    "        all_midi = [os.path.join(dp, f)\n",
    "                    for dp, _, files in os.walk(root)\n",
    "                    for f in files if f.lower().endswith(\".mid\") or f.lower().endswith(\".midi\")]\n",
    "        random.shuffle(all_midi)\n",
    "        n = len(all_midi)\n",
    "        train, val, test = (\n",
    "            all_midi[: int(0.8 * n)],\n",
    "            all_midi[int(0.8 * n): int(0.9 * n)],\n",
    "            all_midi[int(0.9 * n):],\n",
    "        )\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "train_files, val_files, test_files = build_split_lists(DATA_ROOT, SPLIT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LEN = 8          # drop sequences shorter than this after filtering\n",
    "\n",
    "# Create dataset class which can do instrument filtering on the fly\n",
    "class FilteredDatasetMIDI(DatasetMIDI):\n",
    "    def __init__(self, *args, allowed_programs: List[int], **kw):\n",
    "        super().__init__(*args, **kw)\n",
    "        self.allowed_program_ids = {\n",
    "            tokenizer[f\"Program_{p}\"] for p in allowed_programs\n",
    "        }\n",
    "\n",
    "    def _filter_tokens(self, ids: List[int]):\n",
    "        keep, ok = [], False\n",
    "        for tid in ids:\n",
    "            if tid in ALL_PROGRAM_TOKEN_IDS:         # program-change\n",
    "                ok = tid in self.allowed_program_ids\n",
    "                if ok:\n",
    "                    keep.append(tid)\n",
    "            elif ok:                                 # events of allowed track\n",
    "                keep.append(tid)\n",
    "        return keep\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Keep resampling indices until we get a dict with a **non-None**\n",
    "        'input_ids' tensor that remains ‚â• MIN_LEN tokens after filtering.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            sample = super().__getitem__(idx)               # may be None\n",
    "            if sample is None or sample.get(\"input_ids\") is None:\n",
    "                idx = random.randrange(len(self)); continue\n",
    "\n",
    "            ids = sample[\"input_ids\"]\n",
    "            if isinstance(ids, torch.Tensor):\n",
    "                ids = ids.tolist()\n",
    "\n",
    "            filtered = self._filter_tokens(ids)\n",
    "            if len(filtered) < MIN_LEN:\n",
    "                idx = random.randrange(len(self)); continue\n",
    "\n",
    "            sample[\"input_ids\"] = torch.tensor(filtered, dtype=torch.long)\n",
    "            return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final train and validation datasets\n",
    "train_dataset = FilteredDatasetMIDI(\n",
    "    files_paths=train_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    allowed_programs=allowed_programs,\n",
    ")\n",
    "val_dataset = FilteredDatasetMIDI(\n",
    "    files_paths=val_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    allowed_programs=allowed_programs,\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collator)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e5d3a",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19282054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Layer LSTM Impolementation\n",
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66096b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop to train multi instrument LSTM\n",
    "def train(model, train_loader, val_loader, vocab_size, num_epochs=10, lr=0.001, device=DEVICE):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- training ----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch[\"input_ids\"].to(device)\n",
    "            inputs, targets = batch[:, :-1], batch[:, 1:]\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(inputs)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train = total_train_loss / len(train_loader)\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch[\"input_ids\"].to(device)\n",
    "                inputs, targets = batch[:, :-1], batch[:, 1:]\n",
    "                logits, _ = model(inputs)\n",
    "                loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} ‚Äî train: {avg_train:.4f} | val: {avg_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6945b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "vocab = tokenizer.vocab_size\n",
    "model = MusicRNN(vocab, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS)\n",
    "\n",
    "print(\"‚Üí starting training on\", len(train_files), \"files (multi‚Äëinstrument)\")\n",
    "train(model, train_loader, val_loader, vocab, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460f3fa",
   "metadata": {},
   "source": [
    "Music Generation (Sampling from LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e63db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Smapling method, gives model complete freedom on token generation\n",
    "def sample_free(\n",
    "    model: nn.Module,\n",
    "    tokenizer: REMI,\n",
    "    allowed_programs: List[int],\n",
    "    max_length: int = 1024,\n",
    "    temperature: float = 1.0,\n",
    "    device: str = DEVICE,\n",
    "):\n",
    "    \"\"\"Generate a token sequence that uses ONLY the requested GM program numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    allowed_programs : list[int]\n",
    "        List of MIDI program numbers (0‚Äë127) the piece may contain. Example: [0] for solo\n",
    "        piano, [40, 41, 42] for a string trio.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Build a mask over the vocabulary: 1 for allowed tokens, 0 to ban.\n",
    "    allow_ids = {PROGRAM_TOKEN_IDS[p] for p in allowed_programs if p in PROGRAM_TOKEN_IDS}\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # ---- 3. sampler mask: get rid of token_types_indices ----------\n",
    "    mask = torch.ones(tokenizer.vocab_size, device=device)\n",
    "    for pid in ALL_PROGRAM_TOKEN_IDS:\n",
    "        if pid not in {PROGRAM_TOKEN_IDS[p] for p in allowed_programs}:\n",
    "            mask[pid] = 0.0\n",
    "\n",
    "    \n",
    "    # Seed sequence: <BOS> + first Program token (choose first allowed)\n",
    "    # first_prog = allowed_programs[0]\n",
    "    generated = [tokenizer[\"BOS_None\"], allowed_programs[0]]\n",
    "\n",
    "    input_tok = torch.tensor([[generated[-1]]], device=device)\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits, hidden = model(input_tok, hidden)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1) * mask  # zero out banned programs\n",
    "        if probs.sum() == 0:\n",
    "            # catastrophic: mask wiped all mass, fall back to uniform over allowed\n",
    "            probs = mask / mask.sum()\n",
    "        else:\n",
    "            probs = probs / probs.sum()  # renorm\n",
    "        next_tok = torch.multinomial(probs, 1).item()\n",
    "        generated.append(next_tok)\n",
    "        if next_tok == tokenizer[\"EOS_None\"]:\n",
    "            break\n",
    "        input_tok = torch.tensor([[next_tok]], device=device)\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c6632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Smapling method, model still has freedom but is insentivized to use varying instruments\n",
    "def sample_weighted(\n",
    "    model: nn.Module,\n",
    "    tokenizer: REMI,\n",
    "    allowed_programs: List[int],\n",
    "    max_length: int = 1024,\n",
    "    section_len: int = 256,      # distance between forced program switches\n",
    "    temperature: float = 1.0,\n",
    "    device: str = DEVICE,\n",
    "):\n",
    "    \"\"\"Guaranteed multi-instrument generator.\n",
    "\n",
    "    The sequence is partitioned into sections of `section_len` tokens.\n",
    "    At the start of each section we *force-insert* the next unused Program\n",
    "    token from `allowed_programs`.  All other Program tokens remain masked\n",
    "    out, so the model can‚Äôt override the schedule.\n",
    "    \"\"\"\n",
    "    model.to(device); model.eval()\n",
    "\n",
    "    # vocab mask that blocks EVERY Program token for now\n",
    "    global_mask = torch.ones(tokenizer.vocab_size, device=device)\n",
    "    for pid in ALL_PROGRAM_TOKEN_IDS:\n",
    "        global_mask[pid] = 0.0\n",
    "\n",
    "    # list of Program ids we will inject\n",
    "    prog_queue = [PROGRAM_TOKEN_IDS[p] for p in allowed_programs]\n",
    "    active_prog_id = prog_queue.pop(0)          # first program is active\n",
    "\n",
    "    generated = [tokenizer[\"BOS_None\"], active_prog_id]\n",
    "    mask = global_mask.clone()\n",
    "    mask[active_prog_id] = 1.0                  # allow notes for current program\n",
    "\n",
    "    input_tok = torch.tensor([[active_prog_id]], device=device)\n",
    "    hidden = None\n",
    "\n",
    "    for t in range(2, max_length):\n",
    "\n",
    "        # --- force a new instrument at section boundaries ----------\n",
    "        if (t % section_len == 0) and prog_queue:\n",
    "            active_prog_id = prog_queue.pop(0)\n",
    "            generated.append(active_prog_id)\n",
    "            mask = global_mask.clone()\n",
    "            mask[active_prog_id] = 1.0\n",
    "            input_tok = torch.tensor([[active_prog_id]], device=device)\n",
    "            hidden = None                            # reset hidden to avoid shock\n",
    "            continue\n",
    "\n",
    "        # --- normal token sampling --------------------------------\n",
    "        logits, hidden = model(input_tok, hidden)\n",
    "        probs = torch.softmax(logits[:, -1, :] / temperature, dim=-1) * mask\n",
    "        probs = probs / probs.sum()\n",
    "        next_tok = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        generated.append(next_tok)\n",
    "        if next_tok == tokenizer[\"EOS_None\"]:\n",
    "            break\n",
    "\n",
    "        input_tok = torch.tensor([[next_tok]], device=device)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b7ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- sampling demo : string quartet (Violin, Viola, Cello)\n",
    "string_programs = [40, 41, 42]  # GM numbers for Violin, Viola, Cello\n",
    "# NOTE: string programs should be program numbers that are in the top 20 instruments (model is trained on)\n",
    "tokens = sample(model, tokenizer, string_programs, max_length=2048*10)\n",
    "midi = tokenizer.decode(tokens)\n",
    "midi.dump_midi(\"multi_RNN.mid\")\n",
    "print(\"Saved multi_RNN.mid (\", len(tokens), \"tokens )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857772a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect generated MIDI content\n",
    "pretty_midi = PrettyMIDI(\"multi_RNN.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03905b9",
   "metadata": {},
   "source": [
    "Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random midi file across multiple instruments\n",
    "def random_baseline_sample(tokenizer, allowed_programs, seq_len=1024):\n",
    "    \"\"\"\n",
    "    Uniform-random baseline that never strays outside `allowed_programs`.\n",
    "    \"\"\"\n",
    "    rng = random.Random()\n",
    "\n",
    "    # ids for Program tokens we allow\n",
    "    allow_prog_ids = {PROGRAM_TOKEN_IDS[p] for p in allowed_programs}\n",
    "\n",
    "    # every vocab id except ALL program tokens\n",
    "    legal_ids = [tid for tid in range(len(tokenizer))\n",
    "                 if tid not in ALL_PROGRAM_TOKEN_IDS]\n",
    "\n",
    "    # also permit the allowed Program tokens themselves (so a second track may appear)\n",
    "    legal_ids += list(allow_prog_ids)\n",
    "\n",
    "    out = [tokenizer[\"BOS_None\"], PROGRAM_TOKEN_IDS[allowed_programs[0]]]\n",
    "\n",
    "    for _ in range(seq_len - 3):  # reserve one slot for EOS\n",
    "        out.append(rng.choice(legal_ids))\n",
    "\n",
    "    out.append(tokenizer[\"EOS_None\"])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random multi-instrument sample\n",
    "tokens = random_baseline_sample(tokenizer, allowed_programs=[25, 33], seq_len=2048)\n",
    "\n",
    "midi = tokenizer.decode(tokens)\n",
    "midi.dump_midi(\"multi_random_2.mid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea3d80",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Function\n",
    "def compute_instrument_metrics(midi_stream, label):\n",
    "    results = []\n",
    "\n",
    "    for i, part in enumerate(midi_stream.parts):\n",
    "        instr = part.getInstrument(returnDefault=True)\n",
    "        name = instr.instrumentName or f\"Instrument {int((i+1)/2 + 1/2)}\"\n",
    "\n",
    "        if isinstance(instr, instrument.Percussion) or 'drum' in name.lower():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            detected_key = part.analyze('key')\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        allowed_notes = set(p.name for p in detected_key.pitches)\n",
    "\n",
    "        total_notes = 0\n",
    "        nonconforming = 0\n",
    "        velocities = []\n",
    "        pitches = []\n",
    "\n",
    "        for el in part.recurse():\n",
    "            if isinstance(el, note.Note):\n",
    "                total_notes += 1\n",
    "                pitches.append(el.pitch.midi)\n",
    "                velocities.append(el.volume.velocity or 64)\n",
    "                if el.pitch.name not in allowed_notes:\n",
    "                    nonconforming += 1\n",
    "\n",
    "        if total_notes == 0:\n",
    "            continue\n",
    "\n",
    "        key_compliance = 100 * (1 - nonconforming / total_notes)\n",
    "        avg_velocity = statistics.mean(velocities) if velocities else 64\n",
    "        pitch_range = max(pitches) - min(pitches) if len(pitches) >= 2 else 0\n",
    "\n",
    "        results.append({\n",
    "            \"instrument\": name,\n",
    "            \"label\": label,\n",
    "            \"key_compliance\": key_compliance,\n",
    "            \"avg_velocity\": avg_velocity,\n",
    "            \"pitch_range\": pitch_range\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Plotting Function ---\n",
    "\n",
    "def plot_grouped_bar_chart(data, metric_name, title, ylabel):\n",
    "    instruments = sorted(set(d[\"instrument\"] for d in data))\n",
    "    models = sorted(set(d[\"label\"] for d in data))\n",
    "\n",
    "    metric_data = {model: [] for model in models}\n",
    "    for instr in instruments:\n",
    "        for model in models:\n",
    "            match = next((d for d in data if d[\"instrument\"] == instr and d[\"label\"] == model), None)\n",
    "            value = match[metric_name] if match else 0\n",
    "            metric_data[model].append(value)\n",
    "\n",
    "    x = range(len(instruments))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for i, model in enumerate(models):\n",
    "        ax.bar([p + i * width for p in x], metric_data[model], width=width, label=model)\n",
    "\n",
    "    ax.set_xticks([p + width * (len(models) / 2 - 0.5) for p in x])\n",
    "    ax.set_xticklabels(instruments, rotation=45)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Driver Function ---\n",
    "\n",
    "def compare_midi_models(model_file_path, random_file_path, model_label=\"LSTM\", baseline_label=\"Random\"):\n",
    "    try:\n",
    "        model_stream = converter.parse(model_file_path)\n",
    "        random_stream = converter.parse(random_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading MIDI files: {e}\")\n",
    "        return\n",
    "\n",
    "    model_metrics = compute_instrument_metrics(model_stream, model_label)\n",
    "    random_metrics = compute_instrument_metrics(random_stream, baseline_label)\n",
    "    combined = model_metrics + random_metrics\n",
    "\n",
    "    #plot_grouped_bar_chart(combined, \"key_compliance\", \"Key Compliance by Instrument\", \"Key Compliance (%)\")\n",
    "    #plot_grouped_bar_chart(combined, \"avg_velocity\", \"Average Velocity by Instrument\", \"Average Velocity\")\n",
    "    plot_grouped_bar_chart(combined, \"pitch_range\", \"Pitch Range by Instrument\", \"Pitch Range (semitones)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Evaluation Usage\n",
    "\n",
    "model_file = \"generated-multi-instrument-music/multi.mid\"\n",
    "random_file = \"generated-random-instrument-music/random_2.mid\"\n",
    "compare_midi_models(model_file, random_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
