{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892c87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install miditok\n",
    "#!pip install symusic\n",
    "#!pip install glob\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012f5709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from symusic import Score\n",
    "from miditok import REMI, TokenizerConfig\n",
    "\n",
    "from midi2audio import FluidSynth # Import library\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from pretty_midi import PrettyMIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255bf256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Uses 'cuda' if a gpu is detected. Otherwise uses cpu\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Can also set manually\n",
    "#DEVICE = 'cpu'\n",
    "#DEVICE = 'cuda'\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c60d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"maestro-v3.0.0\"            # change if you unpacked elsewhere\n",
    "meta = pd.read_csv(os.path.join(ROOT, \"maestro-v3.0.0.csv\"))\n",
    "\n",
    "def list_midi_files(split):\n",
    "    paths = meta.loc[meta[\"split\"] == split, \"midi_filename\"]\n",
    "    return [os.path.join(ROOT, p) for p in paths]\n",
    "\n",
    "train_files = list_midi_files(\"train\")        # 962 MIDI files\n",
    "val_files   = list_midi_files(\"validation\")   # 137\n",
    "test_files  = list_midi_files(\"test\")         # 177\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5691ee3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'maestro-v3.0.0\\\\2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R3_2018_wav--1.midi'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'maestro-v3.0.0\\\\2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R3_2018_wav--1.midi'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_files[0])\n",
    "train_files[0].encode('utf-8').decode('utf-8')\n",
    "print(train_files[0].encode('utf-8'))\n",
    "str.encode(train_files[0], 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1fe386",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f30258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "\n",
    "tokenizer = REMI()  # using defaults parameters (constants.py)\n",
    "train_dataset = DatasetMIDI(\n",
    "    files_paths=train_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "test_dataset = DatasetMIDI(\n",
    "    files_paths=test_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35331c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 45)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794e05e",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74c73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, num_heads=8, num_layers=6, dropout=0.1, max_seq_len=1024):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = nn.Parameter(self._generate_positional_encoding(max_seq_len, embedding_dim), requires_grad=False)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embedding_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        x = self.embedding(x) + self.pos_encoder[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_len, d_model):\n",
    "        \"\"\"Creates sinusoidal positional encoding matrix\"\"\"\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097e0ab",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831bd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, vocab_size, num_epochs=20, lr=0.001, device=DEVICE):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --------- Training ---------\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch['input_ids'].to(device)  # (batch_size, seq_length)\n",
    "\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # --------- Validation ---------\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch['input_ids'].to(device)\n",
    "\n",
    "                inputs = batch[:, :-1]\n",
    "                targets = batch[:, 1:]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                targets = targets.reshape(-1)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "896c7d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 4.5337 | Val Loss: 4.4663\n",
      "Epoch 2/20 | Train Loss: 4.5027 | Val Loss: 4.4593\n",
      "Epoch 3/20 | Train Loss: 4.4962 | Val Loss: 4.4514\n",
      "Epoch 4/20 | Train Loss: 4.4362 | Val Loss: 4.2630\n",
      "Epoch 5/20 | Train Loss: 4.3251 | Val Loss: 4.4517\n",
      "Epoch 6/20 | Train Loss: 4.4915 | Val Loss: 4.4505\n",
      "Epoch 7/20 | Train Loss: 4.4898 | Val Loss: 4.4470\n",
      "Epoch 8/20 | Train Loss: 4.4873 | Val Loss: 4.4457\n",
      "Epoch 9/20 | Train Loss: 4.4874 | Val Loss: 4.4484\n",
      "Epoch 10/20 | Train Loss: 4.4864 | Val Loss: 4.4470\n",
      "Epoch 11/20 | Train Loss: 4.4864 | Val Loss: 4.4459\n",
      "Epoch 12/20 | Train Loss: 4.4859 | Val Loss: 4.4449\n",
      "Epoch 13/20 | Train Loss: 4.4860 | Val Loss: 4.4458\n",
      "Epoch 14/20 | Train Loss: 4.4863 | Val Loss: 4.4440\n",
      "Epoch 15/20 | Train Loss: 4.4850 | Val Loss: 4.4450\n",
      "Epoch 16/20 | Train Loss: 4.4851 | Val Loss: 4.4451\n",
      "Epoch 17/20 | Train Loss: 4.4856 | Val Loss: 4.4490\n",
      "Epoch 18/20 | Train Loss: 4.4847 | Val Loss: 4.4462\n",
      "Epoch 19/20 | Train Loss: 4.4839 | Val Loss: 4.4457\n",
      "Epoch 20/20 | Train Loss: 4.4837 | Val Loss: 4.4462\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "\n",
    "model = MusicTransformer(vocab_size, embedding_dim=256, num_heads=8, num_layers=6)\n",
    "train(model, train_loader, test_loader, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb8e86",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "81f48fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, start_token, tokenizer, max_length=512, temperature=1.0, device=DEVICE):\n",
    "    model.eval()\n",
    "\n",
    "    # Build ID → string mapping\n",
    "    if hasattr(tokenizer, 'vocab') and isinstance(tokenizer.vocab, dict):\n",
    "        id_to_token = {v: k for k, v in tokenizer.vocab.items()}\n",
    "    elif hasattr(tokenizer, '_vocab'):\n",
    "        id_to_token = {i: tok for i, tok in enumerate(tokenizer._vocab)}\n",
    "    else:\n",
    "        raise RuntimeError(\"Tokenizer vocab not found\")\n",
    "\n",
    "    generated = [start_token]\n",
    "    input_seq = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    while len(generated) < max_length:\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_seq)\n",
    "            next_logits = logits[0, -1] / temperature\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        token_str = id_to_token.get(next_token, \"\")\n",
    "\n",
    "        # Always add bar/position/timeshift\n",
    "        if token_str.startswith((\"Bar\", \"TimeShift\", \"Position\")):\n",
    "            generated.append(next_token)\n",
    "\n",
    "        # If it's a pitch, follow with velocity and duration\n",
    "        elif token_str.startswith(\"Pitch_\"):\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # Sample a Velocity token\n",
    "            velocity_ids = [i for i, tok in id_to_token.items() if tok.startswith(\"Velocity_\")]\n",
    "            generated.append(random.choice(velocity_ids))\n",
    "\n",
    "            # Sample a Duration token\n",
    "            duration_ids = [i for i, tok in id_to_token.items() if tok.startswith(\"Duration_\")]\n",
    "            generated.append(random.choice(duration_ids))\n",
    "\n",
    "        # Stop on EOS or PAD\n",
    "        if token_str in (\"EOS_None\"):\n",
    "            break\n",
    "\n",
    "        input_seq = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "09fb02fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token sequence:\n",
      "[1, 203, 54, 102, 164, 47, 96, 137, 197, 4, 194, 58, 114, 131, 211, 29, 102, 126, 218, 218, 63, 116, 179, 41, 96, 178, 51, 96, 183, 208, 203, 22, 115, 174, 21, 102, 187, 27, 100, 132, 195, 77, 101, 146, 36, 102, 133, 52, 116, 139, 190, 190, 48, 102, 180, 63, 106, 175, 192, 193, 206, 195, 197, 217, 211, 49, 104, 132, 45, 98, 143, 83, 97, 185, 45, 98, 135, 200, 215, 49, 110, 153, 195, 40, 101, 156, 53, 108, 150, 199, 51, 110, 185, 52, 109, 140, 45, 96, 171, 46, 111, 132, 192, 50, 106, 183, 197, 220, 48, 102, 174, 39, 111, 150, 208, 208, 214, 4, 51, 117, 148, 212, 42, 96, 179, 60, 98, 133, 220, 16, 110, 128, 62, 124, 127, 59, 106, 127, 41, 100, 180, 35, 113, 141, 34, 107, 163, 70, 107, 147, 4, 41, 114, 139, 39, 120, 166, 50, 124, 135, 39, 96, 131, 55, 115, 134, 51, 107, 132, 204, 51, 99, 160, 51, 112, 169, 215, 194, 38, 123, 133, 62, 100, 171, 44, 124, 139, 50, 117, 180, 4, 39, 106, 174, 198, 206, 194, 4, 194, 52, 102, 166, 204, 58, 106, 177, 36, 101, 172, 200, 212, 46, 118, 183, 49, 103, 130, 46, 121, 126, 27, 110, 168, 14, 119, 169, 53, 103, 173, 51, 107, 162, 214, 40, 124, 141, 47, 121, 166, 200, 195, 203, 41, 121, 142, 25, 103, 162, 210, 4, 216, 217, 4, 21, 112, 174, 202, 40, 109, 137, 25, 98, 136, 79, 102, 163, 29, 109, 149, 39, 120, 175, 44, 105, 169, 25, 124, 169, 55, 108, 149, 220, 39, 122, 175, 216, 217, 34, 103, 126, 61, 124, 127, 30, 103, 156, 219, 192, 213, 29, 94, 150, 4, 51, 101, 147, 46, 109, 176, 50, 101, 177, 204, 53, 109, 172, 57, 97, 166, 61, 110, 153, 34, 121, 153, 51, 113, 167, 217, 195, 64, 113, 164, 47, 121, 127, 208, 40, 98, 140, 220, 207, 58, 118, 180, 40, 108, 169, 4, 58, 121, 145, 37, 122, 142, 37, 98, 151, 211, 218, 212, 214, 212, 37, 116, 131, 28, 121, 158, 47, 95, 128, 46, 106, 183, 47, 115, 153, 212, 205, 4, 45, 111, 152, 31, 116, 125, 208, 49, 97, 170, 219, 54, 121, 139, 49, 97, 169, 203, 39, 95, 172, 45, 123, 132, 27, 115, 130, 31, 96, 180, 33, 122, 142, 201, 47, 102, 178, 216, 45, 102, 136, 52, 102, 138, 51, 115, 153, 212, 56, 112, 131, 41, 112, 139, 55, 123, 146, 28, 108, 179, 55, 120, 156, 203, 59, 111, 186, 44, 114, 170, 199, 206, 46, 122, 155, 47, 109, 137, 220, 26, 123, 134, 4, 209, 42, 123, 127, 36, 122, 170, 207, 49, 119, 134, 41, 119, 141, 62, 121, 134, 53, 95, 177, 46, 102, 155, 20, 109, 147, 201, 63, 101, 171, 214, 189, 189, 30, 115, 179, 60, 101, 179, 56, 123, 136, 199, 41, 97, 138, 201, 214, 37, 97, 132, 214, 198, 41, 100, 165, 197, 47, 107, 126, 37, 115, 170, 53, 124, 180, 218, 47, 112, 181, 63, 116, 169, 44, 111, 131, 55, 113, 129, 4, 66, 95, 188, 211, 212, 58, 100, 158, 217, 51, 108, 186, 53, 116, 173, 190, 210, 41, 123, 179, 38, 107, 169, 200, 48, 123, 158, 213, 206, 197, 79, 102, 181, 40, 93, 179, 199, 205, 204, 209, 193, 200, 50, 111, 156, 46, 111, 170, 197, 215, 202, 202, 42, 108, 138, 43, 101, 167, 204, 15, 101, 164, 217, 78, 101, 147, 61, 123, 156, 200, 191, 58, 114, 171, 71, 122, 185, 212, 193, 50, 124, 164, 198, 22, 124, 162, 216, 50, 114, 163, 219, 201, 205, 36, 117, 141, 192, 36, 116, 132, 54, 94, 171, 64, 114, 143, 40, 112, 141, 45, 111, 125, 204, 28, 98, 132, 50, 98, 129, 45, 106, 179, 65, 110, 144, 198, 210, 4, 199, 205, 34, 117, 172, 67, 117, 161, 21, 100, 149, 211, 68, 94, 181, 201, 42, 101, 165, 69, 98, 164, 201, 35, 120, 146, 59, 121, 148, 211, 216, 218, 56, 97, 170, 42, 114, 128, 197, 62, 101, 160, 46, 96, 162, 59, 108, 173, 47, 98, 175, 53, 111, 168, 61, 115, 176, 34, 120, 142, 214, 40, 99, 173, 38, 110, 134, 17, 105, 175, 195, 206, 44, 109, 152, 194, 54, 97, 170, 56, 124, 141, 198, 52, 94, 177, 212, 41, 123, 135, 55, 101, 161, 214, 190, 200, 54, 94, 129, 44, 99, 177, 41, 100, 130, 220, 32, 94, 138, 191, 66, 121, 172, 199, 39, 120, 127, 220, 212, 59, 116, 178, 57, 118, 149, 44, 106, 139, 40, 119, 173, 203, 68, 106, 139, 57, 119, 127, 4, 51, 98, 138, 53, 107, 168, 37, 95, 134, 216, 211, 31, 99, 159, 55, 112, 172, 30, 111, 182, 208, 58, 114, 170, 201, 217, 4, 217, 220, 203, 37, 95, 135, 50, 101, 125, 216, 62, 123, 144, 193, 47, 105, 179, 211, 72, 100, 145, 192, 213, 44, 122, 170, 207, 4, 197, 212, 213, 53, 102, 185, 215, 204, 46, 109, 177, 198, 213, 195, 59, 117, 159, 60, 107, 153, 58, 118, 142, 199, 211, 220, 42, 111, 167, 16, 98, 146, 63, 101, 134, 37, 98, 148, 199, 204, 17, 120, 165, 52, 99, 180, 195, 56, 119, 178, 193, 34, 95, 165, 194, 47, 114, 169, 42, 99, 151, 4, 198, 48, 93, 146, 194, 61, 115, 136, 64, 102, 147, 47, 96, 139, 54, 97, 180, 40, 104, 141, 198, 15, 102, 125, 48, 119, 131, 191, 190, 38, 112, 131, 27, 99, 176, 59, 100, 188, 61, 121, 182, 190, 209, 19, 112, 127, 201, 4, 4, 214, 194, 216, 200, 55, 102, 134, 62, 121, 154, 60, 116, 141, 73, 97, 142, 200, 202, 53, 123, 155, 56, 107, 182, 38, 107, 142, 32, 95, 146, 205, 40, 103, 185, 53, 118, 173, 38, 95, 184, 208, 211, 57, 123, 137, 4, 200]\n",
      "num tokens: 1024\n"
     ]
    }
   ],
   "source": [
    "start_token = tokenizer.special_tokens_ids[1]\n",
    "generated_sequence = sample(model, start_token, tokenizer, max_length=1024)\n",
    "#generated_sequence2 = sample(model, generated_sequence1[-1], tokenizer, max_length=1024)\n",
    "\n",
    "#generated_sequence = generated_sequence1 + generated_sequence2\n",
    "\n",
    "print(\"Generated token sequence:\")\n",
    "print(generated_sequence)\n",
    "print(\"num tokens:\", len(generated_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "049cfb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_score = tokenizer.decode([generated_sequence])\n",
    "output_score.dump_midi(f\"transformer.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ed0e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration (seconds): 44.625\n",
      "Acoustic Grand Piano: 269 notes\n"
     ]
    }
   ],
   "source": [
    "pretty_midi = PrettyMIDI(\"transformer.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_time = 1000\n",
    "for i in range(100):\n",
    "    start_token = tokenizer.special_tokens_ids[1]\n",
    "    generated_sequence = sample(model, start_token, tokenizer, max_length=1024)\n",
    "\n",
    "    output_score = tokenizer.decode([generated_sequence])\n",
    "    output_score.dump_midi(f\"transformer_temp.mid\")\n",
    "\n",
    "    pretty_midi_temp = PrettyMIDI(\"transformer_temp.mid\")\n",
    "    if(pretty_midi_temp.get_end_time() < shortest_time):\n",
    "        output_score = tokenizer.decode([generated_sequence])\n",
    "        output_score.dump_midi(f\"transformer.mid\")\n",
    "        shortest_time = pretty_midi.get_end_time()\n",
    "\n",
    "pretty_midi = PrettyMIDI(\"transformer.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
