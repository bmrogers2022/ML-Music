{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "892c87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install miditok\n",
    "#!pip install symusic\n",
    "#!pip install glob\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "012f5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from symusic import Score\n",
    "from miditok import REMI, TokenizerConfig, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "\n",
    "from midi2audio import FluidSynth # Import library\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from pretty_midi import PrettyMIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "255bf256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Uses 'cuda' if a gpu is detected. Otherwise uses cpu\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Can also set manually\n",
    "#DEVICE = 'cpu'\n",
    "#DEVICE = 'cuda'\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c60d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"maestro-v3.0.0\"            # change if you unpacked elsewhere\n",
    "meta = pd.read_csv(os.path.join(ROOT, \"maestro-v3.0.0.csv\"))\n",
    "\n",
    "def list_midi_files(split):\n",
    "    paths = meta.loc[meta[\"split\"] == split, \"midi_filename\"]\n",
    "    return [os.path.join(ROOT, p) for p in paths]\n",
    "\n",
    "train_files = list_midi_files(\"train\")        # 962 MIDI files\n",
    "val_files   = list_midi_files(\"validation\")   # 137\n",
    "test_files  = list_midi_files(\"test\")         # 177\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5691ee3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'maestro-v3.0.0\\\\2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R3_2018_wav--1.midi'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'maestro-v3.0.0\\\\2018/MIDI-Unprocessed_Chamber3_MID--AUDIO_10_R3_2018_wav--1.midi'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_files[0])\n",
    "train_files[0].encode('utf-8').decode('utf-8')\n",
    "print(train_files[0].encode('utf-8'))\n",
    "str.encode(train_files[0], 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1fe386",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f30258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer parameters\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"use_chords\": True,\n",
    "    \"use_tempos\": True,\n",
    "    \"use_time_signatures\": True,\n",
    "    \"use_key_signatures\": True,\n",
    "}\n",
    "\n",
    "# Create the tokenizer configuration\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "\n",
    "# Initialize the REMI tokenizer with the configuration\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "\n",
    "train_dataset = DatasetMIDI(\n",
    "    files_paths=train_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "test_dataset = DatasetMIDI(\n",
    "    files_paths=test_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35331c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 45)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794e05e",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c74c73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, num_heads=8, num_layers=6, dropout=0.1, max_seq_len=1024):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = nn.Parameter(self._generate_positional_encoding(max_seq_len, embedding_dim), requires_grad=False)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embedding_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        x = self.embedding(x) + self.pos_encoder[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_len, d_model):\n",
    "        \"\"\"Creates sinusoidal positional encoding matrix\"\"\"\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097e0ab",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "831bd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, vocab_size, num_epochs=20, lr=0.001, device=DEVICE):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --------- Training ---------\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch['input_ids'].to(device)  # (batch_size, seq_length)\n",
    "\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # --------- Validation ---------\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch['input_ids'].to(device)\n",
    "\n",
    "                inputs = batch[:, :-1]\n",
    "                targets = batch[:, 1:]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                targets = targets.reshape(-1)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "896c7d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 4.2482 | Val Loss: 4.5187\n",
      "Epoch 2/20 | Train Loss: 4.4970 | Val Loss: 4.4702\n",
      "Epoch 3/20 | Train Loss: 4.5175 | Val Loss: 4.4673\n",
      "Epoch 4/20 | Train Loss: 4.5121 | Val Loss: 4.4685\n",
      "Epoch 5/20 | Train Loss: 4.5075 | Val Loss: 4.4656\n",
      "Epoch 6/20 | Train Loss: 4.5057 | Val Loss: 4.4665\n",
      "Epoch 7/20 | Train Loss: 4.5025 | Val Loss: 4.4625\n",
      "Epoch 8/20 | Train Loss: 4.5037 | Val Loss: 4.4613\n",
      "Epoch 9/20 | Train Loss: 4.5000 | Val Loss: 4.4619\n",
      "Epoch 10/20 | Train Loss: 4.4997 | Val Loss: 4.4632\n",
      "Epoch 11/20 | Train Loss: 4.5014 | Val Loss: 4.4630\n",
      "Epoch 12/20 | Train Loss: 4.4989 | Val Loss: 4.4679\n",
      "Epoch 13/20 | Train Loss: 4.5010 | Val Loss: 4.4582\n",
      "Epoch 14/20 | Train Loss: 4.4987 | Val Loss: 4.4662\n",
      "Epoch 15/20 | Train Loss: 4.4992 | Val Loss: 4.4617\n",
      "Epoch 16/20 | Train Loss: 4.5005 | Val Loss: 4.4605\n",
      "Epoch 17/20 | Train Loss: 4.5000 | Val Loss: 4.4602\n",
      "Epoch 18/20 | Train Loss: 4.4998 | Val Loss: 4.4648\n",
      "Epoch 19/20 | Train Loss: 4.4988 | Val Loss: 4.4602\n",
      "Epoch 20/20 | Train Loss: 4.4992 | Val Loss: 4.4613\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "\n",
    "model = MusicTransformer(vocab_size, embedding_dim=256, num_heads=8, num_layers=6)\n",
    "train(model, train_loader, test_loader, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb8e86",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81f48fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, start_token, tokenizer, max_length=512, temperature=1.0, device=DEVICE):\n",
    "    model.eval()\n",
    "\n",
    "    # Build ID → string mapping\n",
    "    if hasattr(tokenizer, 'vocab') and isinstance(tokenizer.vocab, dict):\n",
    "        id_to_token = {v: k for k, v in tokenizer.vocab.items()}\n",
    "    elif hasattr(tokenizer, '_vocab'):\n",
    "        id_to_token = {i: tok for i, tok in enumerate(tokenizer._vocab)}\n",
    "    else:\n",
    "        raise RuntimeError(\"Tokenizer vocab not found\")\n",
    "\n",
    "    generated = [start_token]\n",
    "    input_seq = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    while len(generated) < max_length:\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_seq)\n",
    "            next_logits = logits[0, -1] / temperature\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        token_str = id_to_token.get(next_token, \"\")\n",
    "\n",
    "        # Always add bar/position/timeshift\n",
    "        if token_str.startswith((\"Bar\", \"TimeShift\", \"Position\")):\n",
    "            generated.append(next_token)\n",
    "\n",
    "        # If it's a pitch, follow with velocity and duration\n",
    "        elif token_str.startswith(\"Pitch_\"):\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # Sample a Velocity token\n",
    "            velocity_ids = [i for i, tok in id_to_token.items() if tok.startswith(\"Velocity_\")]\n",
    "            generated.append(random.choice(velocity_ids))\n",
    "\n",
    "            # Sample a Duration token\n",
    "            duration_ids = [i for i, tok in id_to_token.items() if tok.startswith(\"Duration_\")]\n",
    "            generated.append(random.choice(duration_ids))\n",
    "\n",
    "        # Stop on EOS or PAD\n",
    "        if token_str in (\"EOS_None\"):\n",
    "            break\n",
    "\n",
    "        input_seq = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09fb02fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token sequence:\n",
      "[1, 44, 94, 171, 202, 197, 217, 57, 102, 128, 54, 99, 131, 44, 108, 146, 4, 40, 105, 167, 52, 94, 148, 201, 4, 53, 116, 159, 56, 115, 170, 64, 110, 161, 200, 24, 122, 158, 44, 93, 159, 27, 118, 172, 53, 108, 184, 57, 112, 164, 191, 35, 111, 181, 4, 14, 95, 186, 207, 48, 105, 164, 218, 45, 94, 168, 66, 123, 171, 24, 100, 160, 68, 113, 175, 190, 198, 50, 111, 171, 207, 208, 41, 121, 176, 59, 102, 155, 216, 198, 35, 109, 128, 31, 107, 172, 21, 104, 159, 213, 52, 119, 156, 39, 107, 165, 40, 112, 186, 58, 106, 156, 194, 57, 98, 125, 206, 16, 118, 183, 41, 116, 166, 190, 202, 50, 112, 145, 199, 58, 116, 151, 216, 64, 108, 178, 15, 98, 185, 53, 93, 134, 215, 210, 31, 119, 154, 58, 102, 129, 215, 23, 122, 133, 45, 118, 163, 46, 94, 156, 191, 57, 111, 125, 192, 28, 119, 166, 25, 113, 141, 193, 190, 43, 119, 145, 205, 4, 196, 42, 116, 159, 219, 33, 123, 173, 39, 117, 133, 216, 217, 4, 63, 107, 161, 52, 100, 146, 45, 104, 152, 57, 93, 126, 42, 95, 167, 218, 4, 196, 37, 122, 152, 36, 97, 159, 52, 97, 181, 37, 121, 157, 207, 52, 97, 167, 73, 124, 153, 18, 111, 149, 202, 201, 4, 218, 52, 124, 178, 38, 97, 172, 60, 120, 164, 13, 102, 148, 206, 211, 213, 216, 39, 117, 180, 31, 115, 180, 44, 100, 176, 39, 112, 156, 210, 65, 124, 188, 48, 118, 130, 47, 115, 171, 4, 215, 53, 116, 143, 23, 106, 146, 215, 211, 47, 103, 143, 18, 115, 125, 33, 114, 150, 198, 211, 53, 98, 152, 34, 102, 177, 33, 96, 145, 30, 114, 168, 50, 117, 139, 70, 122, 161, 53, 118, 183, 53, 94, 136, 47, 105, 145, 191, 38, 115, 168, 35, 122, 137, 50, 108, 160, 23, 102, 178, 19, 119, 150, 205, 60, 105, 180, 40, 121, 180, 206, 70, 104, 178, 51, 105, 142, 214, 68, 98, 164, 213, 215, 4, 58, 108, 165, 44, 103, 153, 70, 97, 158, 28, 97, 154, 58, 113, 180, 210, 211, 44, 115, 162, 26, 118, 133, 35, 97, 130, 41, 123, 148, 48, 106, 143, 220, 4, 195, 202, 214, 189, 51, 111, 175, 199, 54, 99, 125, 39, 102, 165, 215, 189, 200, 56, 111, 136, 213, 55, 114, 177, 33, 102, 166, 57, 119, 127, 62, 104, 157, 209, 68, 123, 185, 49, 99, 149, 194, 201, 4, 192, 50, 99, 176, 52, 99, 171, 42, 97, 151, 51, 111, 184, 61, 94, 182, 57, 111, 129, 208, 71, 120, 145, 62, 103, 183, 45, 118, 133, 204, 196, 16, 123, 161, 18, 119, 175, 46, 106, 165, 215, 75, 105, 147, 15, 113, 140, 201, 220, 213, 60, 102, 180, 30, 117, 171, 54, 97, 151, 213, 34, 123, 150, 35, 107, 139, 201, 194, 204, 192, 194, 194, 25, 100, 126, 191, 71, 121, 126, 4, 197, 51, 113, 171, 213, 56, 123, 135, 52, 115, 175, 208, 40, 101, 170, 51, 120, 126, 61, 109, 178, 28, 110, 182, 30, 121, 164, 214, 213, 22, 121, 181, 57, 118, 179, 38, 119, 188, 39, 121, 178, 205, 47, 102, 176, 36, 102, 152, 58, 108, 179, 27, 123, 126, 214, 217, 194, 195, 63, 117, 143, 59, 93, 161, 192, 4, 33, 122, 175, 220, 58, 110, 142, 47, 117, 131, 57, 107, 175, 203, 49, 93, 164, 213, 4, 48, 103, 182, 207, 211, 218, 19, 109, 177, 213, 61, 117, 183, 65, 102, 187, 9, 117, 174, 33, 117, 180, 56, 101, 155, 63, 109, 175, 24, 93, 170, 198, 206, 201, 15, 96, 175, 4, 4, 62, 113, 167, 208, 206, 51, 97, 155, 193, 42, 107, 169, 217, 58, 95, 148, 48, 114, 152, 196, 49, 122, 125, 46, 121, 179, 196, 59, 118, 150, 197, 41, 93, 148, 49, 119, 180, 34, 104, 164, 43, 95, 187, 44, 106, 168, 193, 199, 208, 195, 201, 190, 52, 106, 183, 213, 208, 39, 99, 131, 32, 114, 179, 4, 46, 99, 164, 191, 202, 199, 44, 113, 132, 192, 55, 121, 136, 192, 38, 116, 168, 189, 193, 197, 35, 114, 157, 55, 111, 153, 66, 110, 174, 207, 206, 220, 196, 50, 95, 176, 46, 107, 162, 50, 93, 136, 27, 107, 180, 22, 114, 136, 29, 114, 151, 74, 114, 140, 213, 59, 100, 143, 215, 65, 105, 151, 66, 116, 129, 65, 110, 127, 4, 54, 103, 166, 45, 120, 125, 37, 106, 145, 190, 4, 199, 64, 102, 146, 45, 108, 149, 201, 194, 57, 107, 152, 194, 44, 114, 125, 66, 112, 178, 59, 94, 129, 27, 121, 159, 196, 35, 122, 139, 59, 97, 147, 59, 105, 162, 58, 103, 158, 58, 118, 130, 215, 29, 101, 182, 34, 123, 143, 49, 121, 185, 55, 122, 178, 54, 117, 157, 34, 123, 165, 49, 120, 159, 60, 94, 146, 63, 110, 187, 58, 124, 135, 196, 60, 102, 139, 33, 106, 144, 208, 38, 110, 125, 47, 113, 139, 54, 93, 150, 23, 123, 143, 78, 122, 148, 51, 93, 169, 51, 120, 173, 208, 210, 66, 107, 159, 16, 94, 170, 62, 121, 150, 204, 28, 98, 146, 214, 54, 124, 172, 61, 103, 160, 215, 189, 27, 115, 178, 55, 109, 140, 60, 109, 164, 47, 98, 165, 206, 210, 45, 121, 144, 211, 53, 101, 186, 46, 100, 178, 49, 106, 129, 33, 97, 126, 54, 104, 133, 53, 119, 166, 4, 53, 115, 163, 66, 114, 163, 195, 57, 99, 183, 62, 122, 130, 215, 60, 115, 171, 212, 4, 201, 64, 108, 187, 197, 23, 118, 182, 47, 105, 146, 47, 103, 134, 61, 116, 185, 59, 109, 136, 28, 121, 184, 193, 205, 24, 116, 181, 49, 94, 131, 70, 119, 139, 70, 121, 142, 194, 195, 197, 32, 115, 183, 42, 108, 129, 208, 53, 104, 169, 54, 107, 170, 36, 94, 126, 61, 103, 167]\n",
      "num tokens: 1024\n"
     ]
    }
   ],
   "source": [
    "start_token = tokenizer.special_tokens_ids[1]\n",
    "generated_sequence = sample(model, start_token, tokenizer, max_length=1024)\n",
    "#generated_sequence2 = sample(model, generated_sequence1[-1], tokenizer, max_length=1024)\n",
    "\n",
    "#generated_sequence = generated_sequence1 + generated_sequence2\n",
    "\n",
    "print(\"Generated token sequence:\")\n",
    "print(generated_sequence)\n",
    "print(\"num tokens:\", len(generated_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "049cfb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_score = tokenizer.decode([generated_sequence])\n",
    "output_score.dump_midi(f\"transformer.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ed0e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration (seconds): 48.10782449999999\n",
      "Acoustic Grand Piano: 279 notes\n"
     ]
    }
   ],
   "source": [
    "pretty_midi = PrettyMIDI(\"transformer.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d8c3aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration (seconds): 48.10782449999999\n",
      "Acoustic Grand Piano: 267 notes\n"
     ]
    }
   ],
   "source": [
    "shortest_time = 1000\n",
    "for i in range(100):\n",
    "    start_token = tokenizer.special_tokens_ids[1]\n",
    "    generated_sequence = sample(model, start_token, tokenizer, max_length=1024)\n",
    "\n",
    "    output_score = tokenizer.decode([generated_sequence])\n",
    "    output_score.dump_midi(f\"transformer_temp.mid\")\n",
    "\n",
    "    pretty_midi_temp = PrettyMIDI(\"transformer_temp.mid\")\n",
    "    if(pretty_midi_temp.get_end_time() < shortest_time):\n",
    "        output_score = tokenizer.decode([generated_sequence])\n",
    "        output_score.dump_midi(f\"transformer.mid\")\n",
    "        shortest_time = pretty_midi.get_end_time()\n",
    "\n",
    "pretty_midi = PrettyMIDI(\"transformer.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
