{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install miditok\n",
    "#!pip install symusic\n",
    "#!pip install glob\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from symusic import Score\n",
    "from miditok import REMI, TokenizerConfig\n",
    "\n",
    "from midi2audio import FluidSynth # Import library\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from pretty_midi import PrettyMIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bf256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses 'cuda' if a gpu is detected. Otherwise uses cpu\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Can also set manually\n",
    "#DEVICE = 'cpu'\n",
    "#DEVICE = 'cuda'\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"maestro-v3.0.0\"            # change if you unpacked elsewhere\n",
    "meta = pd.read_csv(os.path.join(ROOT, \"maestro-v3.0.0.csv\"))\n",
    "\n",
    "def list_midi_files(split):\n",
    "    paths = meta.loc[meta[\"split\"] == split, \"midi_filename\"]\n",
    "    return [os.path.join(ROOT, p) for p in paths]\n",
    "\n",
    "train_files = list_midi_files(\"train\")        # 962 MIDI files\n",
    "val_files   = list_midi_files(\"validation\")   # 137\n",
    "test_files  = list_midi_files(\"test\")         # 177\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_files[0])\n",
    "train_files[0].encode('utf-8').decode('utf-8')\n",
    "print(train_files[0].encode('utf-8'))\n",
    "str.encode(train_files[0], 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1fe386",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "\n",
    "tokenizer = REMI()  # using defaults parameters (constants.py)\n",
    "train_dataset = DatasetMIDI(\n",
    "    files_paths=train_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "test_dataset = DatasetMIDI(\n",
    "    files_paths=test_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35331c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794e05e",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, num_heads=8, num_layers=6, dropout=0.1, max_seq_len=1024):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = nn.Parameter(self._generate_positional_encoding(max_seq_len, embedding_dim), requires_grad=False)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embedding_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        x = self.embedding(x) + self.pos_encoder[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_len, d_model):\n",
    "        \"\"\"Creates sinusoidal positional encoding matrix\"\"\"\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097e0ab",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831bd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, vocab_size, num_epochs=20, lr=0.001, device=DEVICE):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --------- Training ---------\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch['input_ids'].to(device)  # (batch_size, seq_length)\n",
    "\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # --------- Validation ---------\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch['input_ids'].to(device)\n",
    "\n",
    "                inputs = batch[:, :-1]\n",
    "                targets = batch[:, 1:]\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                targets = targets.reshape(-1)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "\n",
    "model = MusicTransformer(vocab_size, embedding_dim=256, num_heads=8, num_layers=6)\n",
    "train(model, train_loader, test_loader, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb8e86",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f48fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, start_token, tokenizer, max_length=512, temperature=1.0, device=DEVICE):\n",
    "    model.eval()\n",
    "\n",
    "    # Build ID â†’ string mapping\n",
    "    if hasattr(tokenizer, 'vocab') and isinstance(tokenizer.vocab, dict):\n",
    "        id_to_token = {v: k for k, v in tokenizer.vocab.items()}\n",
    "    elif hasattr(tokenizer, '_vocab'):\n",
    "        id_to_token = {i: tok for i, tok in enumerate(tokenizer._vocab)}\n",
    "    else:\n",
    "        raise RuntimeError(\"Tokenizer vocab not found\")\n",
    "\n",
    "    generated = [start_token]\n",
    "    input_seq = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    while len(generated) < max_length:\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_seq)\n",
    "            next_logits = logits[0, -1] / temperature\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        token_str = id_to_token.get(next_token, \"\")\n",
    "\n",
    "        # Always add bar/position/timeshift\n",
    "        if token_str.startswith((\"Bar\", \"TimeShift\", \"Position\")):\n",
    "            generated.append(next_token)\n",
    "\n",
    "        # If it's a pitch, follow with velocity and duration\n",
    "        elif token_str.startswith(\"Pitch_\"):\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # Sample a Velocity token\n",
    "            velocity_ids = [i for i, tok in id_to_token.items() if tok.startswith(\"Velocity_\")]\n",
    "            generated.append(random.choice(velocity_ids))\n",
    "\n",
    "            # Sample a Duration token\n",
    "            duration_ids = [i for i, tok in id_to_token.items() if tok.startswith(\"Duration_\")]\n",
    "            generated.append(random.choice(duration_ids))\n",
    "\n",
    "        # Stop on EOS or PAD\n",
    "        if token_str in (\"EOS_None\"):\n",
    "            break\n",
    "\n",
    "        input_seq = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = tokenizer.special_tokens_ids[1]\n",
    "generated_sequence = sample(model, start_token, tokenizer, max_length=1024)\n",
    "#generated_sequence2 = sample(model, generated_sequence1[-1], tokenizer, max_length=1024)\n",
    "\n",
    "#generated_sequence = generated_sequence1 + generated_sequence2\n",
    "\n",
    "print(\"Generated token sequence:\")\n",
    "print(generated_sequence)\n",
    "print(\"num tokens:\", len(generated_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049cfb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_score = tokenizer.decode([generated_sequence])\n",
    "output_score.dump_midi(f\"transformer.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_midi = PrettyMIDI(\"transformer.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_time = 1000\n",
    "for i in range(100):\n",
    "    start_token = tokenizer.special_tokens_ids[1]\n",
    "    generated_sequence = sample(model, start_token, tokenizer, max_length=1024)\n",
    "\n",
    "    output_score = tokenizer.decode([generated_sequence])\n",
    "    output_score.dump_midi(f\"transformer_temp.mid\")\n",
    "\n",
    "    pretty_midi_temp = PrettyMIDI(\"transformer_temp.mid\")\n",
    "    if(pretty_midi_temp.get_end_time() < shortest_time):\n",
    "        output_score = tokenizer.decode([generated_sequence])\n",
    "        output_score.dump_midi(f\"transformer.mid\")\n",
    "        shortest_time = pretty_midi.get_end_time()\n",
    "\n",
    "pretty_midi = PrettyMIDI(\"transformer.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
