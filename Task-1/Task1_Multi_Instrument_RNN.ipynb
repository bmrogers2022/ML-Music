{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99274683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ------------ Dataset -------------\n",
    "# Replace with the folder that contains **multi‑instrument** MIDI files.\n",
    "# For quick tests you can point to the Lakh Cleaned dataset root.\n",
    "\n",
    "# https://www.kaggle.com/datasets/imsparsh/lakh-midi-clean?resource=download\n",
    "DATA_ROOT = \"lakh_midi\"\n",
    "\n",
    "# If you have a CSV with train/val/test splits, load it here; otherwise we create one.\n",
    "SPLIT_CSV = None  # path to optional CSV with columns [filepath, split]\n",
    "\n",
    "MAX_SEQ_LEN = 1024           # split long pieces into chunks of this many tokens\n",
    "BATCH_SIZE  = 4\n",
    "NUM_EPOCHS  = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "EMBED_DIM  = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563946df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMI with Program‑Change events → single flattened event stream\n",
    "config = TokenizerConfig(\n",
    "    use_programs=True,      # ← this is the key flag\n",
    "    one_token_stream=True,  # keeps your single‑stream RNN forward\n",
    ")\n",
    "\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "# Helper set for fast lookup during constrained sampling\n",
    "PROGRAM_TOKEN_IDS = {\n",
    "    p: tokenizer[f\"Program_{p}\"]\n",
    "    for p in range(128)\n",
    "    if f\"Program_{p}\" in tokenizer  # safety for unseen programs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_split_lists(root: str, csv: Optional[str] = None):\n",
    "    \"\"\"Return lists of MIDI paths grouped by split.\"\"\"\n",
    "    if csv:\n",
    "        meta = pd.read_csv(csv)\n",
    "        train = meta.loc[meta[\"split\"] == \"train\", \"filepath\"].tolist()\n",
    "        val   = meta.loc[meta[\"split\"] == \"validation\", \"filepath\"].tolist()\n",
    "        test  = meta.loc[meta[\"split\"] == \"test\", \"filepath\"].tolist()\n",
    "    else:\n",
    "        # Simple 80‑10‑10 random split over *.mid files in the root\n",
    "        all_midi = [os.path.join(dp, f)\n",
    "                    for dp, _, files in os.walk(root)\n",
    "                    for f in files if f.lower().endswith(\".mid\") or f.lower().endswith(\".midi\")]\n",
    "        random.shuffle(all_midi)\n",
    "        n = len(all_midi)\n",
    "        train, val, test = (\n",
    "            all_midi[: int(0.8 * n)],\n",
    "            all_midi[int(0.8 * n): int(0.9 * n)],\n",
    "            all_midi[int(0.9 * n):],\n",
    "        )\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files, test_files = build_split_lists(DATA_ROOT, SPLIT_CSV)\n",
    "\n",
    "train_dataset = DatasetMIDI(\n",
    "    files_paths=train_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "val_dataset = DatasetMIDI(\n",
    "    files_paths=val_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")\n",
    "\n",
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collator)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, vocab_size, num_epochs=20, lr=0.001, device=DEVICE):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- training ----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch[\"input_ids\"].to(device)\n",
    "            inputs, targets = batch[:, :-1], batch[:, 1:]\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(inputs)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train = total_train_loss / len(train_loader)\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch[\"input_ids\"].to(device)\n",
    "                inputs, targets = batch[:, :-1], batch[:, 1:]\n",
    "                logits, _ = model(inputs)\n",
    "                loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} — train: {avg_train:.4f} | val: {avg_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_programs(\n",
    "    model: nn.Module,\n",
    "    tokenizer: REMI,\n",
    "    allowed_programs: List[int],\n",
    "    max_length: int = 1024,\n",
    "    temperature: float = 1.0,\n",
    "    device: str = DEVICE,\n",
    "):\n",
    "    \"\"\"Generate a token sequence that uses ONLY the requested GM program numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    allowed_programs : list[int]\n",
    "        List of MIDI program numbers (0‑127) the piece may contain. Example: [0] for solo\n",
    "        piano, [40, 41, 42] for a string trio.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Build a mask over the vocabulary: 1 for allowed tokens, 0 to ban.\n",
    "    allow_ids = {PROGRAM_TOKEN_IDS[p] for p in allowed_programs if p in PROGRAM_TOKEN_IDS}\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    mask = torch.ones(vocab_size, device=device)\n",
    "    for pid in PROGRAM_TOKEN_IDS.values():\n",
    "        if pid not in allow_ids:\n",
    "            mask[pid] = 0.0\n",
    "    \n",
    "    # Seed sequence: <BOS> + first Program token (choose first allowed)\n",
    "    first_prog = allowed_programs[0]\n",
    "    generated = [tokenizer[\"BOS_None\"], PROGRAM_TOKEN_IDS[first_prog]]\n",
    "\n",
    "    input_tok = torch.tensor([[generated[-1]]], device=device)\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits, hidden = model(input_tok, hidden)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1) * mask  # zero out banned programs\n",
    "        if probs.sum() == 0:\n",
    "            # catastrophic: mask wiped all mass, fall back to uniform over allowed\n",
    "            probs = mask / mask.sum()\n",
    "        else:\n",
    "            probs = probs / probs.sum()  # renorm\n",
    "        next_tok = torch.multinomial(probs, 1).item()\n",
    "        generated.append(next_tok)\n",
    "        if next_tok == tokenizer[\"EOS_None\"]:\n",
    "            break\n",
    "        input_tok = torch.tensor([[next_tok]], device=device)\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6320bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.vocab_size\n",
    "model = MusicRNN(vocab, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS)\n",
    "\n",
    "print(\"→ starting training on\", len(train_files), \"files (multi‑instrument)\")\n",
    "train(model, train_loader, val_loader, vocab)\n",
    "\n",
    "# ---- sampling demo : string quartet (Violin, Viola, Cello)\n",
    "string_programs = [40, 41, 42]  # GM numbers for Violin, Viola, Cello\n",
    "tokens = sample_with_programs(model, tokenizer, string_programs, max_length=2048)\n",
    "midi = tokenizer.tokens_to_midi([tokens])\n",
    "midi.dump_midi(\"sample_quartet.mid\")\n",
    "print(\"Saved sample_quartet.mid (\", len(tokens), \"tokens )\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
