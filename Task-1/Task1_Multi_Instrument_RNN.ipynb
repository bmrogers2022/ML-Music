{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99274683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "\n",
    "from collections import defaultdict\n",
    "from symusic import Score\n",
    "\n",
    "from pretty_midi import PrettyMIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f2b100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "# ------------ Dataset -------------\n",
    "# Download from: https://www.kaggle.com/datasets/imsparsh/lakh-midi-clean?resource=download\n",
    "# Unzip and name dataset folder root \"lakh-midi\"\n",
    "DATA_ROOT = \"lakh-midi\"\n",
    "\n",
    "# If you have a CSV with train/val/test splits, load it here; otherwise we create one.\n",
    "SPLIT_CSV = None  # path to optional CSV with columns [filepath, split]\n",
    "\n",
    "MAX_SEQ_LEN = 1024           # split long pieces into chunks of this many tokens\n",
    "BATCH_SIZE  = 4\n",
    "NUM_EPOCHS  = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "EMBED_DIM  = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d96033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-20 instruments:\n",
      "  Program   0: 13455 tracks\n",
      "  Program  25: 6211 tracks\n",
      "  Program  29: 6127 tracks\n",
      "  Program  48: 6038 tracks\n",
      "  Program  33: 5848 tracks\n",
      "  Program  30: 5434 tracks\n",
      "  Program  27: 5334 tracks\n",
      "  Program  35: 4211 tracks\n",
      "  Program  52: 3894 tracks\n",
      "  Program  26: 3379 tracks\n",
      "  Program  28: 3315 tracks\n",
      "  Program  49: 3258 tracks\n",
      "  Program  24: 3250 tracks\n",
      "  Program  32: 2877 tracks\n",
      "  Program   1: 2780 tracks\n",
      "  Program  53: 2542 tracks\n",
      "  Program  65: 2420 tracks\n",
      "  Program  50: 2389 tracks\n",
      "  Program  61: 2293 tracks\n",
      "  Program  73: 2135 tracks\n"
     ]
    }
   ],
   "source": [
    "def collect_program_counts(root: str):\n",
    "    counts = defaultdict(int)\n",
    "    midi_files = [os.path.join(dp, f)\n",
    "                  for dp, _, files in os.walk(root)\n",
    "                  for f in files if f.lower().endswith((\".mid\", \".midi\"))]\n",
    "    for fp in midi_files:\n",
    "        try:\n",
    "            score = Score(fp)\n",
    "            for track in score.tracks:\n",
    "                if track.is_drum:                 # skip channel-10 drums\n",
    "                    continue\n",
    "                counts[track.program] += 1\n",
    "        except Exception:\n",
    "            continue                              # skip unreadable files\n",
    "    return counts, midi_files\n",
    "\n",
    "program_counts, all_midi_files = collect_program_counts(DATA_ROOT)\n",
    "TOP20 = sorted(program_counts.items(), key=lambda kv: kv[1], reverse=True)[:20]\n",
    "allowed_programs = [p for p, _ in TOP20]\n",
    "\n",
    "print(\"Top-20 instruments:\")\n",
    "for prog, cnt in TOP20:\n",
    "    print(f\"  Program {prog:3d}: {cnt} tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563946df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\musicgen\\lib\\site-packages\\miditok\\tokenizations\\remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
      "  super().__init__(tokenizer_config, params)\n"
     ]
    }
   ],
   "source": [
    "# REMI with Program‑Change events → single flattened event stream\n",
    "config = TokenizerConfig(\n",
    "    use_programs=True,      # ← this is the key flag\n",
    "    one_token_stream=True,  # keeps your single‑stream RNN forward\n",
    ")\n",
    "\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "# Helper set for fast lookup during constrained sampling\n",
    "PROGRAM_TOKEN_IDS = {\n",
    "    p: tokenizer[f\"Program_{p}\"]\n",
    "    for p in range(128)\n",
    "    if f\"Program_{p}\" in tokenizer  # safety for unseen programs\n",
    "}\n",
    "\n",
    "ALL_PROGRAM_TOKEN_IDS = set(PROGRAM_TOKEN_IDS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42e2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_split_lists(root: str, csv: Optional[str] = None):\n",
    "    \"\"\"Return lists of MIDI paths grouped by split.\"\"\"\n",
    "    if csv:\n",
    "        meta = pd.read_csv(csv)\n",
    "        train = meta.loc[meta[\"split\"] == \"train\", \"filepath\"].tolist()\n",
    "        val   = meta.loc[meta[\"split\"] == \"validation\", \"filepath\"].tolist()\n",
    "        test  = meta.loc[meta[\"split\"] == \"test\", \"filepath\"].tolist()\n",
    "    else:\n",
    "        # Simple 80‑10‑10 random split over *.mid files in the root\n",
    "        all_midi = [os.path.join(dp, f)\n",
    "                    for dp, _, files in os.walk(root)\n",
    "                    for f in files if f.lower().endswith(\".mid\") or f.lower().endswith(\".midi\")]\n",
    "        random.shuffle(all_midi)\n",
    "        n = len(all_midi)\n",
    "        train, val, test = (\n",
    "            all_midi[: int(0.8 * n)],\n",
    "            all_midi[int(0.8 * n): int(0.9 * n)],\n",
    "            all_midi[int(0.9 * n):],\n",
    "        )\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "train_files, val_files, test_files = build_split_lists(DATA_ROOT, SPLIT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79420be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13782\n"
     ]
    }
   ],
   "source": [
    "print(len(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da77b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LEN = 8          # drop sequences shorter than this after filtering\n",
    "\n",
    "class FilteredDatasetMIDI(DatasetMIDI):\n",
    "    def __init__(self, *args, allowed_programs: List[int], **kw):\n",
    "        super().__init__(*args, **kw)\n",
    "        self.allowed_program_ids = {\n",
    "            tokenizer[f\"Program_{p}\"] for p in allowed_programs\n",
    "        }\n",
    "\n",
    "    def _filter_tokens(self, ids: List[int]):\n",
    "        keep, ok = [], False\n",
    "        for tid in ids:\n",
    "            if tid in ALL_PROGRAM_TOKEN_IDS:         # program-change\n",
    "                ok = tid in self.allowed_program_ids\n",
    "                if ok:\n",
    "                    keep.append(tid)\n",
    "            elif ok:                                 # events of allowed track\n",
    "                keep.append(tid)\n",
    "        return keep\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Keep resampling indices until we get a dict with a **non-None**\n",
    "        'input_ids' tensor that remains ≥ MIN_LEN tokens after filtering.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            sample = super().__getitem__(idx)               # may be None\n",
    "            if sample is None or sample.get(\"input_ids\") is None:\n",
    "                idx = random.randrange(len(self)); continue\n",
    "\n",
    "            ids = sample[\"input_ids\"]\n",
    "            if isinstance(ids, torch.Tensor):\n",
    "                ids = ids.tolist()\n",
    "\n",
    "            filtered = self._filter_tokens(ids)\n",
    "            if len(filtered) < MIN_LEN:\n",
    "                idx = random.randrange(len(self)); continue\n",
    "\n",
    "            sample[\"input_ids\"] = torch.tensor(filtered, dtype=torch.long)\n",
    "            return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d8861dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FilteredDatasetMIDI(\n",
    "    files_paths=train_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    allowed_programs=allowed_programs,\n",
    ")\n",
    "val_dataset = FilteredDatasetMIDI(\n",
    "    files_paths=val_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    allowed_programs=allowed_programs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb16937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collator)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1735db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31f0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, vocab_size, num_epochs=10, lr=0.001, device=DEVICE):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- training ----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch[\"input_ids\"].to(device)\n",
    "            inputs, targets = batch[:, :-1], batch[:, 1:]\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(inputs)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train = total_train_loss / len(train_loader)\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch[\"input_ids\"].to(device)\n",
    "                inputs, targets = batch[:, :-1], batch[:, 1:]\n",
    "                logits, _ = model(inputs)\n",
    "                loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} — train: {avg_train:.4f} | val: {avg_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6320bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ starting training on 13782 files (multi‑instrument)\n",
      "Epoch 1/10 — train: 1.6422 | val: 1.3758\n",
      "Epoch 2/10 — train: 1.2645 | val: 1.2165\n",
      "Epoch 3/10 — train: 1.1526 | val: 1.1582\n",
      "Epoch 4/10 — train: 1.0847 | val: 1.1211\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab_size\n",
    "model = MusicRNN(vocab, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS)\n",
    "\n",
    "print(\"→ starting training on\", len(train_files), \"files (multi‑instrument)\")\n",
    "train(model, train_loader, val_loader, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_programs(\n",
    "    model: nn.Module,\n",
    "    tokenizer: REMI,\n",
    "    allowed_programs: List[int],\n",
    "    max_length: int = 1024,\n",
    "    temperature: float = 1.0,\n",
    "    device: str = DEVICE,\n",
    "):\n",
    "    \"\"\"Generate a token sequence that uses ONLY the requested GM program numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    allowed_programs : list[int]\n",
    "        List of MIDI program numbers (0‑127) the piece may contain. Example: [0] for solo\n",
    "        piano, [40, 41, 42] for a string trio.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Build a mask over the vocabulary: 1 for allowed tokens, 0 to ban.\n",
    "    allow_ids = {PROGRAM_TOKEN_IDS[p] for p in allowed_programs if p in PROGRAM_TOKEN_IDS}\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # ---- 3. sampler mask: get rid of token_types_indices ----------\n",
    "    mask = torch.ones(tokenizer.vocab_size, device=device)\n",
    "    for pid in ALL_PROGRAM_TOKEN_IDS:\n",
    "        if pid not in {PROGRAM_TOKEN_IDS[p] for p in allowed_programs}:\n",
    "            mask[pid] = 0.0\n",
    "\n",
    "    \n",
    "    # Seed sequence: <BOS> + first Program token (choose first allowed)\n",
    "    # first_prog = allowed_programs[0]\n",
    "    generated = [tokenizer[\"BOS_None\"], allowed_programs[0]]\n",
    "\n",
    "    input_tok = torch.tensor([[generated[-1]]], device=device)\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits, hidden = model(input_tok, hidden)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1) * mask  # zero out banned programs\n",
    "        if probs.sum() == 0:\n",
    "            # catastrophic: mask wiped all mass, fall back to uniform over allowed\n",
    "            probs = mask / mask.sum()\n",
    "        else:\n",
    "            probs = probs / probs.sum()  # renorm\n",
    "        next_tok = torch.multinomial(probs, 1).item()\n",
    "        generated.append(next_tok)\n",
    "        if next_tok == tokenizer[\"EOS_None\"]:\n",
    "            break\n",
    "        input_tok = torch.tensor([[next_tok]], device=device)\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe1848",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_with_programs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ---- sampling demo : string quartet (Violin, Viola, Cello)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#string_programs = [40, 41, 42]  # GM numbers for Violin, Viola, Cello\u001b[39;00m\n\u001b[0;32m      3\u001b[0m string_programs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m33\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43msample_with_programs\u001b[49m(model, tokenizer, string_programs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      5\u001b[0m midi \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens)\n\u001b[0;32m      6\u001b[0m midi\u001b[38;5;241m.\u001b[39mdump_midi(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_RNN.mid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_with_programs' is not defined"
     ]
    }
   ],
   "source": [
    "# ---- sampling demo : string quartet (Violin, Viola, Cello)\n",
    "#string_programs = [40, 41, 42]  # GM numbers for Violin, Viola, Cello\n",
    "string_programs = [24, 33]\n",
    "tokens = sample_with_programs(model, tokenizer, string_programs, max_length=2048*6)\n",
    "midi = tokenizer.decode(tokens)\n",
    "midi.dump_midi(\"multi_RNN.mid\")\n",
    "print(\"Saved multi_RNN.mid (\", len(tokens), \"tokens )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration (seconds): 9.4375\n",
      "Acoustic Grand Piano: 1 notes\n",
      "Drums: 100 notes\n"
     ]
    }
   ],
   "source": [
    "pretty_midi = PrettyMIDI(\"multi_RNN.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c077c",
   "metadata": {},
   "source": [
    "  Program   Instrument\n",
    "  0:\n",
    "  25:\n",
    "  29:\n",
    "  48:\n",
    "  33:\n",
    "  30:\n",
    "  27:\n",
    "  35:\n",
    "  26:\n",
    "  28:\n",
    "  49:\n",
    "  24:\n",
    "  32:\n",
    "   1:\n",
    "  53:\n",
    "  65:\n",
    "  50:\n",
    "  61:\n",
    "  73:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca4a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration (seconds): 40.5\n",
      "String Ensembles 2: 442 notes\n",
      "Drums: 29 notes\n",
      "Flute: 28 notes\n"
     ]
    }
   ],
   "source": [
    "midi = PrettyMIDI(\"multi_RNN.mid\")\n",
    "print(\"Duration (seconds):\", midi.get_end_time())\n",
    "for i, instrument in enumerate(midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f6376",
   "metadata": {},
   "source": [
    "File Instruments\n",
    "\n",
    "multi_RNN_11: \n",
    "    Duration (seconds): 15.0\n",
    "    Bright Acoustic Piano: 33 notes\n",
    "    Electric Bass (finger): 49 notes\n",
    "    Drums: 79 notes\n",
    "\n",
    "multi_RNN_12:\n",
    "    Duration (seconds): 10.9375\n",
    "    Acoustic Guitar (nylon): 156 notes\n",
    "    Electric Bass (finger): 23 notes\n",
    "    Drums: 37 notes\n",
    "\n",
    "multi_RNN_14:\n",
    "    Duration (seconds): 15.25\n",
    "    Electric Bass (finger): 47 notes\n",
    "    Drums: 141 notes\n",
    "    Acoustic Guitar (nylon): 57 notes    \n",
    "\n",
    "multi_RNN_15:\n",
    "    Duration (seconds): 40.5\n",
    "    String Ensembles 2: 442 notes\n",
    "    Drums: 29 notes\n",
    "    Flute: 28 notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
