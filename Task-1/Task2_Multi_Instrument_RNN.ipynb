{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99274683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "\n",
    "from collections import defaultdict\n",
    "from symusic import Score\n",
    "\n",
    "from pretty_midi import PrettyMIDI, program_to_instrument_name\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "from music21 import converter, note, key, stream, meter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "# ------------ Dataset -------------\n",
    "# Download from: https://www.kaggle.com/datasets/imsparsh/lakh-midi-clean?resource=download\n",
    "# Unzip and name dataset folder root \"lakh-midi\"\n",
    "DATA_ROOT = \"lakh-midi\"\n",
    "\n",
    "# If you have a CSV with train/val/test splits, load it here; otherwise we create one.\n",
    "SPLIT_CSV = None  # path to optional CSV with columns [filepath, split]\n",
    "\n",
    "MAX_SEQ_LEN = 1024           # split long pieces into chunks of this many tokens\n",
    "BATCH_SIZE  = 4\n",
    "NUM_EPOCHS  = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "EMBED_DIM  = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d96033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_program_counts(root: str):\n",
    "    counts = defaultdict(int)\n",
    "    midi_files = [os.path.join(dp, f)\n",
    "                  for dp, _, files in os.walk(root)\n",
    "                  for f in files if f.lower().endswith((\".mid\", \".midi\"))]\n",
    "    for fp in midi_files:\n",
    "        try:\n",
    "            score = Score(fp)\n",
    "            for track in score.tracks:\n",
    "                if track.is_drum:                 # skip channel-10 drums\n",
    "                    continue\n",
    "                counts[track.program] += 1\n",
    "        except Exception:\n",
    "            continue                              # skip unreadable files\n",
    "    return counts, midi_files\n",
    "\n",
    "program_counts, all_midi_files = collect_program_counts(DATA_ROOT)\n",
    "TOP20 = sorted(program_counts.items(), key=lambda kv: kv[1], reverse=True)[:20]\n",
    "allowed_programs = [p for p, _ in TOP20]\n",
    "\n",
    "print(\"Top-20 instruments:\")\n",
    "print(f\"{'GM#':>4}  {'Name':30}  Tracks\")\n",
    "print(\"-\" * 46)\n",
    "for prog, cnt in TOP20:\n",
    "    name = program_to_instrument_name(prog)   # e.g. 0 → Acoustic Grand Piano\n",
    "    print(f\"{prog:>4}  {name:30}  {cnt:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563946df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMI with Program‑Change events → single flattened event stream\n",
    "config = TokenizerConfig(\n",
    "    use_programs=True,      # ← this is the key flag\n",
    "    one_token_stream=True,  # keeps your single‑stream RNN forward\n",
    ")\n",
    "\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "# Helper set for fast lookup during constrained sampling\n",
    "PROGRAM_TOKEN_IDS = {\n",
    "    p: tokenizer[f\"Program_{p}\"]\n",
    "    for p in range(128)\n",
    "    if f\"Program_{p}\" in tokenizer  # safety for unseen programs\n",
    "}\n",
    "\n",
    "ALL_PROGRAM_TOKEN_IDS = set(PROGRAM_TOKEN_IDS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_split_lists(root: str, csv: Optional[str] = None):\n",
    "    \"\"\"Return lists of MIDI paths grouped by split.\"\"\"\n",
    "    if csv:\n",
    "        meta = pd.read_csv(csv)\n",
    "        train = meta.loc[meta[\"split\"] == \"train\", \"filepath\"].tolist()\n",
    "        val   = meta.loc[meta[\"split\"] == \"validation\", \"filepath\"].tolist()\n",
    "        test  = meta.loc[meta[\"split\"] == \"test\", \"filepath\"].tolist()\n",
    "    else:\n",
    "        # Simple 80‑10‑10 random split over *.mid files in the root\n",
    "        all_midi = [os.path.join(dp, f)\n",
    "                    for dp, _, files in os.walk(root)\n",
    "                    for f in files if f.lower().endswith(\".mid\") or f.lower().endswith(\".midi\")]\n",
    "        random.shuffle(all_midi)\n",
    "        n = len(all_midi)\n",
    "        train, val, test = (\n",
    "            all_midi[: int(0.8 * n)],\n",
    "            all_midi[int(0.8 * n): int(0.9 * n)],\n",
    "            all_midi[int(0.9 * n):],\n",
    "        )\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "train_files, val_files, test_files = build_split_lists(DATA_ROOT, SPLIT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79420be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da77b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LEN = 8          # drop sequences shorter than this after filtering\n",
    "\n",
    "class FilteredDatasetMIDI(DatasetMIDI):\n",
    "    def __init__(self, *args, allowed_programs: List[int], **kw):\n",
    "        super().__init__(*args, **kw)\n",
    "        self.allowed_program_ids = {\n",
    "            tokenizer[f\"Program_{p}\"] for p in allowed_programs\n",
    "        }\n",
    "\n",
    "    def _filter_tokens(self, ids: List[int]):\n",
    "        keep, ok = [], False\n",
    "        for tid in ids:\n",
    "            if tid in ALL_PROGRAM_TOKEN_IDS:         # program-change\n",
    "                ok = tid in self.allowed_program_ids\n",
    "                if ok:\n",
    "                    keep.append(tid)\n",
    "            elif ok:                                 # events of allowed track\n",
    "                keep.append(tid)\n",
    "        return keep\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Keep resampling indices until we get a dict with a **non-None**\n",
    "        'input_ids' tensor that remains ≥ MIN_LEN tokens after filtering.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            sample = super().__getitem__(idx)               # may be None\n",
    "            if sample is None or sample.get(\"input_ids\") is None:\n",
    "                idx = random.randrange(len(self)); continue\n",
    "\n",
    "            ids = sample[\"input_ids\"]\n",
    "            if isinstance(ids, torch.Tensor):\n",
    "                ids = ids.tolist()\n",
    "\n",
    "            filtered = self._filter_tokens(ids)\n",
    "            if len(filtered) < MIN_LEN:\n",
    "                idx = random.randrange(len(self)); continue\n",
    "\n",
    "            sample[\"input_ids\"] = torch.tensor(filtered, dtype=torch.long)\n",
    "            return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8861dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FilteredDatasetMIDI(\n",
    "    files_paths=train_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    allowed_programs=allowed_programs,\n",
    ")\n",
    "val_dataset = FilteredDatasetMIDI(\n",
    "    files_paths=val_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    allowed_programs=allowed_programs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collator)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, vocab_size, num_epochs=10, lr=0.001, device=DEVICE):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # ---- training ----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch[\"input_ids\"].to(device)\n",
    "            inputs, targets = batch[:, :-1], batch[:, 1:]\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(inputs)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train = total_train_loss / len(train_loader)\n",
    "\n",
    "        # ---- validation ----\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch[\"input_ids\"].to(device)\n",
    "                inputs, targets = batch[:, :-1], batch[:, 1:]\n",
    "                logits, _ = model(inputs)\n",
    "                loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} — train: {avg_train:.4f} | val: {avg_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6320bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.vocab_size\n",
    "model = MusicRNN(vocab, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS)\n",
    "\n",
    "print(\"→ starting training on\", len(train_files), \"files (multi‑instrument)\")\n",
    "train(model, train_loader, val_loader, vocab, num_epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_programs(\n",
    "    model: nn.Module,\n",
    "    tokenizer: REMI,\n",
    "    allowed_programs: List[int],\n",
    "    max_length: int = 1024,\n",
    "    temperature: float = 1.0,\n",
    "    device: str = DEVICE,\n",
    "):\n",
    "    \"\"\"Generate a token sequence that uses ONLY the requested GM program numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    allowed_programs : list[int]\n",
    "        List of MIDI program numbers (0‑127) the piece may contain. Example: [0] for solo\n",
    "        piano, [40, 41, 42] for a string trio.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Build a mask over the vocabulary: 1 for allowed tokens, 0 to ban.\n",
    "    allow_ids = {PROGRAM_TOKEN_IDS[p] for p in allowed_programs if p in PROGRAM_TOKEN_IDS}\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # ---- 3. sampler mask: get rid of token_types_indices ----------\n",
    "    mask = torch.ones(tokenizer.vocab_size, device=device)\n",
    "    for pid in ALL_PROGRAM_TOKEN_IDS:\n",
    "        if pid not in {PROGRAM_TOKEN_IDS[p] for p in allowed_programs}:\n",
    "            mask[pid] = 0.0\n",
    "\n",
    "    \n",
    "    # Seed sequence: <BOS> + first Program token (choose first allowed)\n",
    "    # first_prog = allowed_programs[0]\n",
    "    generated = [tokenizer[\"BOS_None\"], allowed_programs[0]]\n",
    "\n",
    "    input_tok = torch.tensor([[generated[-1]]], device=device)\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits, hidden = model(input_tok, hidden)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1) * mask  # zero out banned programs\n",
    "        if probs.sum() == 0:\n",
    "            # catastrophic: mask wiped all mass, fall back to uniform over allowed\n",
    "            probs = mask / mask.sum()\n",
    "        else:\n",
    "            probs = probs / probs.sum()  # renorm\n",
    "        next_tok = torch.multinomial(probs, 1).item()\n",
    "        generated.append(next_tok)\n",
    "        if next_tok == tokenizer[\"EOS_None\"]:\n",
    "            break\n",
    "        input_tok = torch.tensor([[next_tok]], device=device)\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b15809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(\n",
    "    model: nn.Module,\n",
    "    tokenizer: REMI,\n",
    "    allowed_programs: List[int],\n",
    "    max_length: int = 1024,\n",
    "    section_len: int = 256,      # distance between forced program switches\n",
    "    temperature: float = 1.0,\n",
    "    device: str = DEVICE,\n",
    "):\n",
    "    \"\"\"Guaranteed multi-instrument generator.\n",
    "\n",
    "    The sequence is partitioned into sections of `section_len` tokens.\n",
    "    At the start of each section we *force-insert* the next unused Program\n",
    "    token from `allowed_programs`.  All other Program tokens remain masked\n",
    "    out, so the model can’t override the schedule.\n",
    "    \"\"\"\n",
    "    model.to(device); model.eval()\n",
    "\n",
    "    # vocab mask that blocks EVERY Program token for now\n",
    "    global_mask = torch.ones(tokenizer.vocab_size, device=device)\n",
    "    for pid in ALL_PROGRAM_TOKEN_IDS:\n",
    "        global_mask[pid] = 0.0\n",
    "\n",
    "    # list of Program ids we will inject\n",
    "    prog_queue = [PROGRAM_TOKEN_IDS[p] for p in allowed_programs]\n",
    "    active_prog_id = prog_queue.pop(0)          # first program is active\n",
    "\n",
    "    generated = [tokenizer[\"BOS_None\"], active_prog_id]\n",
    "    mask = global_mask.clone()\n",
    "    mask[active_prog_id] = 1.0                  # allow notes for current program\n",
    "\n",
    "    input_tok = torch.tensor([[active_prog_id]], device=device)\n",
    "    hidden = None\n",
    "\n",
    "    for t in range(2, max_length):\n",
    "\n",
    "        # --- force a new instrument at section boundaries ----------\n",
    "        if (t % section_len == 0) and prog_queue:\n",
    "            active_prog_id = prog_queue.pop(0)\n",
    "            generated.append(active_prog_id)\n",
    "            mask = global_mask.clone()\n",
    "            mask[active_prog_id] = 1.0\n",
    "            input_tok = torch.tensor([[active_prog_id]], device=device)\n",
    "            hidden = None                            # reset hidden to avoid shock\n",
    "            continue\n",
    "\n",
    "        # --- normal token sampling --------------------------------\n",
    "        logits, hidden = model(input_tok, hidden)\n",
    "        probs = torch.softmax(logits[:, -1, :] / temperature, dim=-1) * mask\n",
    "        probs = probs / probs.sum()\n",
    "        next_tok = torch.multinomial(probs, 1).item()\n",
    "\n",
    "        generated.append(next_tok)\n",
    "        if next_tok == tokenizer[\"EOS_None\"]:\n",
    "            break\n",
    "\n",
    "        input_tok = torch.tensor([[next_tok]], device=device)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- sampling demo : string quartet (Violin, Viola, Cello)\n",
    "#string_programs = [40, 41, 42]  # GM numbers for Violin, Viola, Cello\n",
    "string_programs = [25, 33]\n",
    "tokens = sample(model, tokenizer, string_programs, max_length=2048*10)\n",
    "midi = tokenizer.decode(tokens)\n",
    "midi.dump_midi(\"multi_RNN.mid\")\n",
    "print(\"Saved multi_RNN.mid (\", len(tokens), \"tokens )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_midi = PrettyMIDI(\"multi_RNN.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c077c",
   "metadata": {},
   "source": [
    "Top-20 instruments:\n",
    "\n",
    " GM#  Name                            Tracks\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "   0  Acoustic Grand Pian\n",
    "\n",
    "  25  Acoustic Guitar (steel)\n",
    "\n",
    "  29  Overdriven Guitar\n",
    "\n",
    "  48  String Ensemble 1\n",
    "\n",
    "  33  Electric Bass (finger)\n",
    "\n",
    "  30  Distortion Guitar\n",
    "\n",
    "  27  Electric Guitar (clean)\n",
    "  \n",
    "  35  Fretless Bass\n",
    "\n",
    "  52  Choir Aahs\n",
    "\n",
    "  26  Electric Guitar (jazz)\n",
    "\n",
    "  28  Electric Guitar (muted)\n",
    "\n",
    "  49  String Ensemble 2\n",
    "\n",
    "  24  Acoustic Guitar (nylon)\n",
    "\n",
    "  32  Acoustic Bass\n",
    "\n",
    "   1  Bright Acoustic Piano\n",
    "\n",
    "  53  Voice Oohs\n",
    "\n",
    "  65  Alto Sax\n",
    "\n",
    "  50  Synth Strings 1\n",
    "\n",
    "  61  Brass Section\n",
    "\n",
    "  73  Flute\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f6376",
   "metadata": {},
   "source": [
    "File Instruments\n",
    "\n",
    "multi_RNN_11: \n",
    "    Duration (seconds): 15.0\n",
    "    Bright Acoustic Piano: 33 notes\n",
    "    Electric Bass (finger): 49 notes\n",
    "    Drums: 79 notes\n",
    "\n",
    "multi_RNN_12:\n",
    "    Duration (seconds): 10.9375\n",
    "    Acoustic Guitar (nylon): 156 notes\n",
    "    Electric Bass (finger): 23 notes\n",
    "    Drums: 37 notes\n",
    "\n",
    "multi_RNN_14:\n",
    "    Duration (seconds): 15.25\n",
    "    Electric Bass (finger): 47 notes\n",
    "    Drums: 141 notes\n",
    "    Acoustic Guitar (nylon): 57 notes    \n",
    "\n",
    "multi_RNN_15:\n",
    "    Duration (seconds): 40.5\n",
    "    String Ensembles 2: 442 notes\n",
    "    Drums: 29 notes\n",
    "    Flute: 28 notes\n",
    "\n",
    "16\n",
    "Duration (seconds): 21.1875\n",
    "Acoustic Grand Piano: 1 notes\n",
    "Acoustic Guitar (nylon): 12 notes\n",
    "Drums: 161 notes\n",
    "\n",
    "17\n",
    "Duration (seconds): 70.25\n",
    "Acoustic Guitar (nylon): 57 notes\n",
    "Drums: 447 notes\n",
    "Electric Bass (finger): 13 notes\n",
    "Choir Aahs: 55 notes\n",
    "\n",
    "18\n",
    "distortion guitar\n",
    "alto sax\n",
    "string ensemble\n",
    "\n",
    "19\n",
    "Duration (seconds): 24.5\n",
    "Acoustic Bass: 14 notes\n",
    "Drums: 70 notes\n",
    "Brass Section: 34 notes\n",
    "Acoustic Grand Piano: 163 notes\n",
    "\n",
    "20\n",
    "Duration (seconds): 39.75\n",
    "Acoustic Grand Piano: 54 notes\n",
    "Acoustic Guitar (steel): 53 notes\n",
    "Drums: 13 notes\n",
    "String Ensembles 1: 55 notes\n",
    "Choir Aahs: 45 notes\n",
    "Acoustic Bass: 27 notes\n",
    "\n",
    "22\n",
    "Duration (seconds): 20.25\n",
    "SynthStrings 1: 63 notes\n",
    "Acoustic Bass: 95 notes\n",
    "Drums: 135 notes\n",
    "\n",
    "23\n",
    "Duration (seconds): 32.3125\n",
    "Electric Guitar (jazz): 48 notes\n",
    "Drums: 31 notes\n",
    "Alto Sax: 27 notes\n",
    "Bright Acoustic Piano: 234 notes\n",
    "\n",
    "24\n",
    "Duration (seconds): 20.75\n",
    "Electric Guitar (jazz): 43 notes\n",
    "Drums: 8 notes\n",
    "Alto Sax: 9 notes\n",
    "\n",
    "25\n",
    "Duration (seconds): 16.125\n",
    "Electric Guitar (jazz): 57 notes\n",
    "Drums: 194 notes\n",
    "Alto Sax: 157 notes\n",
    "\n",
    "26\n",
    "Duration (seconds): 34.25\n",
    "Acoustic Guitar (steel): 54 notes\n",
    "Electric Bass (finger): 78 notes\n",
    "Drums: 236 notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fa30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_baseline_sample(tokenizer, allowed_programs, seq_len=1024):\n",
    "    \"\"\"\n",
    "    Uniform-random baseline that never strays outside `allowed_programs`.\n",
    "    \"\"\"\n",
    "    rng = random.Random()\n",
    "\n",
    "    # ids for Program tokens we allow\n",
    "    allow_prog_ids = {PROGRAM_TOKEN_IDS[p] for p in allowed_programs}\n",
    "\n",
    "    # every vocab id except ALL program tokens\n",
    "    legal_ids = [tid for tid in range(len(tokenizer))\n",
    "                 if tid not in ALL_PROGRAM_TOKEN_IDS]\n",
    "\n",
    "    # also permit the allowed Program tokens themselves (so a second track may appear)\n",
    "    legal_ids += list(allow_prog_ids)\n",
    "\n",
    "    out = [tokenizer[\"BOS_None\"], PROGRAM_TOKEN_IDS[allowed_programs[0]]]\n",
    "\n",
    "    for _ in range(seq_len - 3):  # reserve one slot for EOS\n",
    "        out.append(rng.choice(legal_ids))\n",
    "\n",
    "    out.append(tokenizer[\"EOS_None\"])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cebce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = random_baseline_sample(tokenizer, allowed_programs=[25, 33], seq_len=2048)\n",
    "\n",
    "midi = tokenizer.decode(tokens)\n",
    "midi.dump_midi(\"multi_random_2.mid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e8c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_midi = PrettyMIDI(\"multi_random_2.mid\")\n",
    "print(\"Duration (seconds):\", pretty_midi.get_end_time())\n",
    "for i, instrument in enumerate(pretty_midi.instruments):\n",
    "    print(f\"{instrument.name or 'Unnamed'}:\", len(instrument.notes), \"notes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
